{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for installing the packages for the 1st time use !pip install [package name]\n",
    "import os, re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from html import unescape\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "\n",
    "import pickle\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95980c64",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT140_DATA_DIR = 'Sentiment140.data' # sentiment 140 data set saved here\n",
    "DG_DATA_DIR = 'D_G data' # D&G data set saved here\n",
    "OUTPUT_DIR = 'output' # intermediate output and models saved here\n",
    "FIGURES_DIR = 'figures' # figures saved here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a4afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output and figure directories\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "if not os.path.exists(FIGURES_DIR):\n",
    "    os.makedirs(FIGURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc6b8e",
   "metadata": {
    "id": "babc6b8e"
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(os.path.join(SENTIMENT140_DATA_DIR, \"training.1600000.processed.noemoticon.csv\"), encoding = \"latin-1\",names=[\"predictions\",\"id\",\"timestamp\", \"query\", \"user\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d90d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = dftrain.sample(100000, random_state=42) # only use 100,000 of the 1,600,000 training tweets to save time in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ac2ec",
   "metadata": {
    "id": "8a1ac2ec",
    "outputId": "3248a21b-4b98-45be-f32a-12acd706be9c"
   },
   "outputs": [],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe885adf",
   "metadata": {
    "id": "fe885adf"
   },
   "outputs": [],
   "source": [
    "dftest = pd.read_csv(os.path.join(SENTIMENT140_DATA_DIR, \"testdata.manual.2009.06.14.csv\"), encoding = \"latin-1\",names=[\"predictions\",\"id\",\"timestamp\", \"query\", \"user\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1c268",
   "metadata": {
    "id": "2ad1c268",
    "outputId": "e92bcf44-b584-42a0-9e30-62dd8d5d6bee"
   },
   "outputs": [],
   "source": [
    "dftest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e67a5",
   "metadata": {
    "id": "709e67a5"
   },
   "outputs": [],
   "source": [
    "df_chopsticks = pd.read_csv(os.path.join(DG_DATA_DIR, \"dolcegabbana_chopsticks_mentions_daily_expanded.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chopsticks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a37ba7",
   "metadata": {
    "id": "f6a37ba7"
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(os.path.join(DG_DATA_DIR, \"dolcegabbana_mentions_daily_all.csv\"), lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40b1de",
   "metadata": {
    "id": "bf40b1de",
    "outputId": "f88be774-ffa2-4d8e-c215-4d44b808d3bb"
   },
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a5240",
   "metadata": {
    "id": "e44a5240"
   },
   "outputs": [],
   "source": [
    "# load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d8b14",
   "metadata": {
    "id": "d37d8b14"
   },
   "outputs": [],
   "source": [
    "# helper function for pre-processing/cleaning a tweet\n",
    "def preprocessor(tweet):\n",
    "    tweet = re.sub (r'@[A-Za-z0-9_]+', '_AT_USER_', tweet) # replace @X with _AT_USER_\n",
    "    tweet = re.sub (r'#[A-Za-z0-9_]+', '_HASHTAG_', tweet) # replace #X with _HASTHAG_\n",
    "    tweet = re.sub (r'^RT[\\s]+', '', tweet) # remove RT (retweet) at the start of the tweet\n",
    "    tweet = unescape(tweet) # unescape the HTML\n",
    "    tweet = tweet.lower() # make everything lowercase\n",
    "    return tweet\n",
    "\n",
    "# helper function for tokenization of a tweet\n",
    "def tokenizer(tweet):\n",
    "    tokens = nlp(tweet) # this processes the tweet text  \n",
    "    # only keep tokens (lemmatized) that are alphanumeric (including \"-\" and \"_\") and not a stop word, or represent an emoji\n",
    "    tokens = [t.lemma_ for t in tokens if (re.match(\"^[a-zA-Z0-9_-]*$\", t.text) and not t.is_stop and len(t.text) > 2) or t.text in UNICODE_EMOJI]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae265729",
   "metadata": {
    "id": "ae265729",
    "outputId": "b13a7046-7069-45b5-951e-53a5bc5b8711"
   },
   "outputs": [],
   "source": [
    "corpus_chopsticks = list(df_chopsticks['text']) # a list of tweets\n",
    "corpus_all = list(df_all['text']) # a list of tweets\n",
    "corpus_test = list(dftest['text']) # a list of tweets\n",
    "corpus_train = list(dftrain['text']) # a list of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029acce",
   "metadata": {
    "id": "2029acce"
   },
   "outputs": [],
   "source": [
    "y_train = list(dftrain['predictions'])\n",
    "y_test = list(dftest['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8630be93",
   "metadata": {
    "id": "8630be93"
   },
   "source": [
    "# Count vectorizer transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf6347",
   "metadata": {
    "id": "2abf6347"
   },
   "outputs": [],
   "source": [
    "model = CountVectorizer(preprocessor=preprocessor, tokenizer=tokenizer, max_features=2000)\n",
    "word_counts_train = model.fit_transform(corpus_train)\n",
    "model_features = model.get_feature_names()\n",
    "fitted_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7960e",
   "metadata": {
    "id": "e3c7960e"
   },
   "outputs": [],
   "source": [
    "word_counts_test = fitted_model.transform(corpus_test)\n",
    "print('count vectorizer completed on corpus_test')\n",
    "\n",
    "word_counts_chopsticks = fitted_model.transform(corpus_chopsticks)\n",
    "print('count vectorizer completed on corpus_chopsticks')\n",
    "\n",
    "word_counts_all = fitted_model.transform(corpus_all)\n",
    "print('count vectorizer completed on corpus_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8fe62",
   "metadata": {},
   "source": [
    "# TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659c8dd",
   "metadata": {
    "id": "9659c8dd"
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_140 = tfidf_transformer.fit_transform(word_counts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9dd1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_transformer.transform(word_counts_test)\n",
    "print('TF-IDF completed on corpus_test')\n",
    "\n",
    "X_chopsticks = tfidf_transformer.transform(word_counts_chopsticks)\n",
    "print('TF-IDF completed on corpus_chopsticks')\n",
    "\n",
    "X_all = tfidf_transformer.transform(word_counts_all)\n",
    "print('TF-IDF completed on corpus_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e919f",
   "metadata": {},
   "source": [
    "# Split Sentiment 140 data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9710f",
   "metadata": {
    "id": "8ac9710f"
   },
   "outputs": [],
   "source": [
    "X_train, X_validation, y_trainsmall, y_validation = train_test_split(X_140, y_train, test_size=0.10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb95db",
   "metadata": {
    "id": "edcb95db"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b8db7",
   "metadata": {},
   "source": [
    "# Save feature matrices, labels, and count vectorizer and TF-IDF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57913c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, 'count_vectorizer'), 'wb') as f:\n",
    "    pickle.dump(fitted_model, f)\n",
    "    \n",
    "with open(os.path.join(OUTPUT_DIR, 'tfidf_transformer'), 'wb') as f:\n",
    "    pickle.dump(tfidf_transformer, f)\n",
    "\n",
    "np.save(os.path.join(OUTPUT_DIR, 'model_features.npy'), model_features)\n",
    "\n",
    "# output of count vectorizer\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'word_counts_train.npz'), word_counts_train, compressed=True)\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'word_counts_test.npz'), word_counts_test, compressed=True)\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'word_counts_chopsticks.npz'), word_counts_chopsticks, compressed=True)\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'word_counts_all.npz'), word_counts_all, compressed=True)\n",
    "\n",
    "# output of TF-IDF\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'X_140.npz'), X_140, compressed=True)\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'X_test.npz'), X_test, compressed=True)\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'X_chopsticks.npz'), X_chopsticks, compressed=True)\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'X_all.npz'), X_all, compressed=True)\n",
    "\n",
    "# after train test split\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'X_train.npz'), X_train, compressed=True)\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'X_validation.npz'), X_validation, compressed=True)\n",
    "\n",
    "# save labels\n",
    "np.save(os.path.join(OUTPUT_DIR, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(OUTPUT_DIR, 'y_trainsmall.npy'), y_trainsmall)\n",
    "np.save(os.path.join(OUTPUT_DIR, 'y_validation.npy'), y_validation)\n",
    "np.save(os.path.join(OUTPUT_DIR, 'y_test.npy'), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
