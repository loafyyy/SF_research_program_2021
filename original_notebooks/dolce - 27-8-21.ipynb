{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7241bb",
   "metadata": {
    "id": "ed7241bb",
    "outputId": "85883cf9-f7fd-486c-fee4-f17af9d43f6b"
   },
   "outputs": [],
   "source": [
    "#for installing the packages for the 1st time use !pip install [package name]\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pip install spacy\n",
    "import spacy\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "!pip install emoji\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "!pip install spacy\n",
    "\n",
    "!pip install scipy\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc6b8e",
   "metadata": {
    "id": "babc6b8e"
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/sentiment140data/training.data.csv\", encoding = \"latin-1\",names=[\"predictions\",\"id\",\"timestamp\", \"query\", \"user\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ac2ec",
   "metadata": {
    "id": "8a1ac2ec",
    "outputId": "3248a21b-4b98-45be-f32a-12acd706be9c"
   },
   "outputs": [],
   "source": [
    "dftrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe885adf",
   "metadata": {
    "id": "fe885adf"
   },
   "outputs": [],
   "source": [
    "dftest = pd.read_csv(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/sentiment140data/testdata.manual.2009.06.14.csv\", encoding = \"latin-1\",names=[\"predictions\",\"id\",\"timestamp\", \"query\", \"user\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1c268",
   "metadata": {
    "id": "2ad1c268",
    "outputId": "e92bcf44-b584-42a0-9e30-62dd8d5d6bee"
   },
   "outputs": [],
   "source": [
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e67a5",
   "metadata": {
    "id": "709e67a5"
   },
   "outputs": [],
   "source": [
    "df_chopsticks = pd.read_csv(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/dolcegabbanadata/dolcegabbana_chopsticks_mentions_daily_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a37ba7",
   "metadata": {
    "id": "f6a37ba7"
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/dolcegabbanadata/dolcegabbana_mentions_daily_all.csv\",   lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d094d4",
   "metadata": {
    "id": "62d094d4",
    "outputId": "a94ae2a3-f183-4932-9c44-f353fa35dda1"
   },
   "outputs": [],
   "source": [
    "df_chopsticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40b1de",
   "metadata": {
    "id": "bf40b1de",
    "outputId": "f88be774-ffa2-4d8e-c215-4d44b808d3bb"
   },
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a5240",
   "metadata": {
    "id": "e44a5240"
   },
   "outputs": [],
   "source": [
    "# load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d8b14",
   "metadata": {
    "id": "d37d8b14"
   },
   "outputs": [],
   "source": [
    "# helper function for pre-processing/cleaning a tweet\n",
    "def preprocessor(tweet):\n",
    "    tweet = re.sub (r'@[A-Za-z0-9_]+', '_AT_USER_', tweet) # replace @X with _AT_USER_\n",
    "    tweet = re.sub (r'#[A-Za-z0-9_]+', '_HASHTAG_', tweet) # replace #X with _HASTHAG_\n",
    "    tweet = re.sub (r'^RT[\\s]+', '', tweet) # remove RT (retweet) at the start of the tweet\n",
    "    tweet = unescape(tweet) # unescape the HTML\n",
    "    tweet = tweet.lower() # make everything lowercase\n",
    "    return tweet\n",
    "\n",
    "# helper function for tokenization of a tweet\n",
    "def tokenizer(tweet):\n",
    "    tokens = nlp(tweet) # this processes the tweet text\n",
    "\n",
    "    # tokenize each tweet and remove stop words and punctuation \n",
    "    tokens = [token for token in tokens if not token.is_stop and not token.is_punct] \n",
    "    \n",
    "    # only keep tokens that are alphanumeric (including \"-\" and \"_\") or represent an emoji\n",
    "    tokens = [t for t in tokens if re.match(\"^[a-zA-Z0-9_-]*$\", t.text) or t.text in UNICODE_EMOJI]\n",
    "\n",
    "    # lematize tokens\n",
    "    tokens = [token.lemma_ for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae265729",
   "metadata": {
    "id": "ae265729",
    "outputId": "b13a7046-7069-45b5-951e-53a5bc5b8711"
   },
   "outputs": [],
   "source": [
    "corpus_chopsticks = list(df_chopsticks['text']) # a list of tweets\n",
    "corpus_chopsticks\n",
    "corpus_all = list(df_all['text']) # a list of tweets\n",
    "corpus_all\n",
    "corpus_test = list(dftest['text']) # a list of tweets\n",
    "corpus_test\n",
    "corpus_train = list(dftrain['text']) # a list of tweets\n",
    "corpus_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029acce",
   "metadata": {
    "id": "2029acce"
   },
   "outputs": [],
   "source": [
    "y_train = list(dftrain['predictions'])\n",
    "y_test = list(dftest['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8630be93",
   "metadata": {
    "id": "8630be93"
   },
   "source": [
    "## SVM\n",
    "manually: (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf6347",
   "metadata": {
    "id": "2abf6347"
   },
   "outputs": [],
   "source": [
    "model = CountVectorizer(preprocessor=preprocessor, tokenizer=tokenizer, max_features=5000)\n",
    "word_counts_train = model.fit_transform(corpus_train)\n",
    "tokens_train = model.get_feature_names()\n",
    "fitted_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302d0a0",
   "metadata": {
    "id": "f302d0a0"
   },
   "outputs": [],
   "source": [
    "model_features = model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7960e",
   "metadata": {
    "id": "e3c7960e"
   },
   "outputs": [],
   "source": [
    "word_counts_test = fitted_model.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e740c",
   "metadata": {
    "id": "819e740c"
   },
   "outputs": [],
   "source": [
    "word_counts_chopsticks = fitted_model.transform(corpus_chopsticks)\n",
    "word_counts_all = fitted_model.transform(corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71dcbcd",
   "metadata": {
    "id": "e71dcbcd"
   },
   "outputs": [],
   "source": [
    "fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659c8dd",
   "metadata": {
    "id": "9659c8dd"
   },
   "outputs": [],
   "source": [
    "# use TF-IDF transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_140 = tfidf_transformer.fit_transform(word_counts_train) # training\n",
    "X_test = tfidf_transformer.transform(word_counts_test) # testing\n",
    "X_chopsticks = tfidf_transformer.transform(word_counts_chopsticks) # training\n",
    "X_all =  tfidf_transformer.transform(word_counts_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df10c40",
   "metadata": {
    "id": "1df10c40"
   },
   "outputs": [],
   "source": [
    "X_140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9710f",
   "metadata": {
    "id": "8ac9710f"
   },
   "outputs": [],
   "source": [
    "X_train, X_validation, y_trainsmall, y_validation = train_test_split(X_140, y_train, test_size=0.10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb95db",
   "metadata": {
    "id": "edcb95db"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ef4b9",
   "metadata": {
    "id": "f72ef4b9"
   },
   "source": [
    "loading: (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23402241",
   "metadata": {
    "id": "23402241"
   },
   "outputs": [],
   "source": [
    "fitted_model = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/model_path\", 'rb'))\n",
    "\n",
    "model_features = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/model_features.npy', allow_pickle=True)\n",
    "\n",
    "word_counts_test = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/word_counts_test.npz')\n",
    "word_counts_chopsticks = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/word_counts_chopsticks.npz')\n",
    "word_counts_all = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/word_counts_all.npz')\n",
    "word_counts_train = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/word_counts_train.npz')\n",
    "\n",
    "X_140 = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/X_140.npz')\n",
    "X_test = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/X_test.npz')\n",
    "X_chopsticks = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/X_chopsticks.npz')\n",
    "X_all = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/X_all.npz')\n",
    "\n",
    "X_train = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/X_train.npz')\n",
    "X_validation = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/X_validation.npz')\n",
    "y_trainsmall = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_trainsmall.npy', allow_pickle=True)\n",
    "y_validation = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_validation.npy', allow_pickle=True)\n",
    "y_test = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_test.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b911a",
   "metadata": {
    "id": "db3b911a",
    "outputId": "4fb9dd52-d85f-4935-8d49-52c5afe0efc8"
   },
   "outputs": [],
   "source": [
    "X_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103d147",
   "metadata": {
    "id": "8103d147"
   },
   "source": [
    "manual svc: (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77199f16",
   "metadata": {
    "id": "77199f16"
   },
   "outputs": [],
   "source": [
    "# fit SVC model with default parameters\n",
    "svc_model = SVC(kernel='linear', C=0.1, probability=True)\n",
    "svc_model.fit(X_train, y_trainsmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9725b",
   "metadata": {
    "id": "8eb9725b"
   },
   "outputs": [],
   "source": [
    "# prediction on training\n",
    "y_train_pred_svc = svc_model.predict(X_train)\n",
    "y_train_proba_svc = svc_model.predict_proba(X_train)\n",
    "\n",
    "# prediction on validation\n",
    "y_validation_pred_svc = svc_model.predict(X_validation)\n",
    "y_validation_proba_svc = svc_model.predict_proba(X_validation)\n",
    "\n",
    "# prediction on chopsticks\n",
    "y_chopsticks_pred_svc = svc_model.predict(X_chopsticks)\n",
    "y_chopsticks_proba_svc = svc_model.predict_proba(X_chopsticks)\n",
    "\n",
    "# prediction on d&g overall\n",
    "y_all_pred_svc = svc_model.predict(X_all)\n",
    "y_all_proba_svc = svc_model.predict_proba(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faccee81",
   "metadata": {
    "id": "faccee81"
   },
   "outputs": [],
   "source": [
    "print('Training performance')\n",
    "print(classification_report(y_trainsmall, y_train_pred_svc))\n",
    "print()\n",
    "print('====================')\n",
    "print()\n",
    "print('Validation performance')\n",
    "print(classification_report(y_validation, y_validation_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e1e8c",
   "metadata": {
    "id": "2b6e1e8c"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(y_validation_proba_svc[:,1])\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.title('Predicted probability distirbution training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded555ce",
   "metadata": {
    "id": "ded555ce"
   },
   "source": [
    "loading svc: (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4d84f",
   "metadata": {
    "id": "18b4d84f"
   },
   "outputs": [],
   "source": [
    "svc_model = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/svc_model\", 'rb'))\n",
    "\n",
    "y_train_pred_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_train_pred_svc.npy', allow_pickle=True)\n",
    "y_train_proba_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_train_proba_svc.npy', allow_pickle=True)\n",
    "y_validation_pred_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_validation_pred_svc.npy', allow_pickle=True)\n",
    "y_validation_proba_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_validation_proba_svc.npy', allow_pickle=True)\n",
    "y_chopsticks_pred_svc_binary = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_chopsticks_pred_svc_binary.npy', allow_pickle=True)\n",
    "y_chopsticks_proba_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_chopsticks_proba_svc.npy', allow_pickle=True)\n",
    "y_all_pred_svc_binary = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_all_pred_svc_binary.npy', allow_pickle=True)\n",
    "y_all_proba_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_all_proba_svc.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25674e31",
   "metadata": {
    "id": "25674e31"
   },
   "source": [
    "manual 140test / probability: (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87895e",
   "metadata": {
    "id": "fd87895e"
   },
   "outputs": [],
   "source": [
    "# prediction on testing (once hyperparameters are tuned)\n",
    "y_test_pred_svc_binary = svc_model.predict(X_test)\n",
    "y_test_proba_svc = svc_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70036c",
   "metadata": {
    "id": "0a70036c"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(y_chopsticks_proba_svc[:,1])\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.title('Predicted probability distribution validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb470e",
   "metadata": {
    "id": "afcb470e"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(y_all_proba_svc[:,1])\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.title('Predicted probability distribution validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08393e7f",
   "metadata": {
    "id": "08393e7f"
   },
   "outputs": [],
   "source": [
    "y_chopsticks_proba_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee0655",
   "metadata": {
    "id": "20ee0655"
   },
   "outputs": [],
   "source": [
    "topacc = 0\n",
    "top_a = 0\n",
    "top_b = 0\n",
    "for i in range (10):\n",
    "    a = 0.5+(i*0.01)\n",
    "    for i in range (10):\n",
    "        y_test_pred_svc_binary = []\n",
    "        b = 0.76-(i*0.01)\n",
    "        for i in range (len(y_test_proba_svc)):\n",
    "            if (y_test_proba_svc[i,1])<a:\n",
    "                y_test_pred_svc_binary.append(0)\n",
    "            elif(y_test_proba_svc[i,1])>b:\n",
    "                y_test_pred_svc_binary.append(4)\n",
    "            else:\n",
    "                y_test_pred_svc_binary.append(2)\n",
    "        accuracy = accuracy_score(y_test, y_test_pred_svc_binary)\n",
    "        if accuracy > topacc:\n",
    "            topacc = accuracy\n",
    "            top_a = a\n",
    "            top_b = b\n",
    "            top_y_pred = y_test_pred_svc_binary\n",
    "            print(\"acc = \" + str(accuracy)+ \" a = \"+str(a)+\" b = \"+str(b))\n",
    "            \n",
    "        \n",
    "print(\"overall top acc = \"+str(topacc)+ \" a = \"+str(top_a)+\" b = \"+str(top_b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19765cc",
   "metadata": {
    "id": "e19765cc"
   },
   "outputs": [],
   "source": [
    "#best cutoff values: 0.5 + 0.69 (positive proba(second column))\n",
    "y_test_pred_svc = top_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcddb77",
   "metadata": {
    "id": "4fcddb77"
   },
   "outputs": [],
   "source": [
    "print('Testing performance')\n",
    "print(classification_report(y_test, y_test_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65757933",
   "metadata": {
    "id": "65757933"
   },
   "outputs": [],
   "source": [
    "y_chopsticks_pred_svc = []\n",
    "for i in range (len(y_chopsticks_proba_svc)):\n",
    "            if (y_chopsticks_proba_svc[i,1])<0.5:\n",
    "                y_chopsticks_pred_svc.append(0)\n",
    "            elif(y_chopsticks_proba_svc[i,1])>0.69:\n",
    "                y_chopsticks_pred_svc.append(4)\n",
    "            else:\n",
    "                y_chopsticks_pred_svc.append(2)\n",
    "y_all_pred_svc = []\n",
    "for i in range (len(y_all_proba_svc)):\n",
    "            if (y_all_proba_svc[i,1])<0.5:\n",
    "                y_all_pred_svc.append(0)\n",
    "            elif(y_all_proba_svc[i,1])>0.69:\n",
    "                y_all_pred_svc.append(4)\n",
    "            else:\n",
    "                y_all_pred_svc.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468051cf",
   "metadata": {
    "id": "468051cf"
   },
   "outputs": [],
   "source": [
    "# see where model predicted wrong\n",
    "for i in range(len(y_test_pred_svc)):\n",
    "  if y_test[i] != y_test_pred_svc[i]:\n",
    "    tweet = corpus_test[i]\n",
    "    print(f'y_true {y_test[i]}, y_pred {y_test_pred_svc[i]}')\n",
    "    print(tweet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660aa2d",
   "metadata": {
    "id": "6660aa2d"
   },
   "source": [
    "loading probability: (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d9626",
   "metadata": {
    "id": "903d9626"
   },
   "outputs": [],
   "source": [
    "y_test_proba_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_test_proba_svc.npy', allow_pickle=True)\n",
    "y_test_pred_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_test_pred_svc.npy', allow_pickle=True)\n",
    "y_chopsticks_pred_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_chopsticks_pred_svc.npy', allow_pickle=True)\n",
    "y_all_pred_svc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/y_all_pred_svc.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2a76f",
   "metadata": {
    "id": "b2a2a76f"
   },
   "source": [
    "manually adding predictions to df: (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ccf5b",
   "metadata": {
    "id": "2e5ccf5b"
   },
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564f069",
   "metadata": {
    "id": "a564f069"
   },
   "outputs": [],
   "source": [
    "df_chopsticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c219d61",
   "metadata": {
    "id": "9c219d61"
   },
   "outputs": [],
   "source": [
    "y_chopsticks_proba_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7aad3",
   "metadata": {
    "id": "02d7aad3"
   },
   "outputs": [],
   "source": [
    "df_chopsticks['pred_label'] = y_chopsticks_pred_svc\n",
    "df_all['pred_label'] = y_all_pred_svc\n",
    "\n",
    "df_chopsticks['pred_score'] = y_chopsticks_proba_svc [:,1]\n",
    "df_all[ 'pred_score'] = y_all_proba_svc [:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dae261",
   "metadata": {
    "id": "c1dae261"
   },
   "outputs": [],
   "source": [
    "# Changing object type column to datetime\n",
    "df_chopsticks['interval_start_date'] = pd.to_datetime(df_chopsticks['interval_start_date'])\n",
    "df_chopsticks['created_at'] = pd.to_datetime(df_chopsticks['created_at'])\n",
    "df_chopsticks['date'] = df_chopsticks['created_at'].dt.date # Creating new column with just the date\n",
    "df_chopsticks['date'] = pd.to_datetime(df_chopsticks['date'])\n",
    "\n",
    "# get week start date to aggregate over weeks\n",
    "df_chopsticks['week_start_date'] = df_chopsticks['date'].apply(lambda x: x - datetime.timedelta(days=x.weekday()))\n",
    "df_chopsticks['week_start_date'] = df_chopsticks['week_start_date'].dt.date\n",
    "df_chopsticks.head()\n",
    "\n",
    "# Changing object type column to datetime\n",
    "df_all['interval_start_date'] = pd.to_datetime(df_all['interval_start_date'])\n",
    "df_all['created_at'] = pd.to_datetime(df_all['created_at'])\n",
    "df_all['date'] = df_all['created_at'].dt.date # Creating new column with just the date\n",
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "\n",
    "# get week start date to aggregate over weeks\n",
    "df_all['week_start_date'] = df_all['date'].apply(lambda x: x - datetime.timedelta(days=x.weekday()))\n",
    "df_all['week_start_date'] = df_all['week_start_date'].dt.date\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5a145",
   "metadata": {
    "id": "88e5a145"
   },
   "outputs": [],
   "source": [
    "# positive is going to be defined as greater than 80% prediction score that it is positive (adjustable based on threshold)\n",
    "df_all_pos = df_all[(df_all['pred_label'] == 4)&(df_all['pred_score']>0.8)]\n",
    "\n",
    "# negative is going to be defined as greater than 80% prediction score that it is negative (adjustable based on threshold)\n",
    "df_all_neg = df_all[(df_all['pred_label'] == 0)&(df_all['pred_score']<0.4)]\n",
    "\n",
    "# positive is going to be defined as greater than 80% prediction score that it is positive (adjustable based on threshold)\n",
    "df_chopsticks_pos = df_chopsticks[(df_chopsticks['pred_label'] == 4)&(df_chopsticks['pred_score']>0.8)]\n",
    "\n",
    "# negative is going to be defined as greater than 80% prediction score that it is negative (adjustable based on threshold)\n",
    "df_chopsticks_neg = df_chopsticks[(df_chopsticks['pred_label'] == 0)&(df_chopsticks['pred_score']<0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a939e0",
   "metadata": {
    "id": "24a939e0"
   },
   "outputs": [],
   "source": [
    "df_all_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab0b89",
   "metadata": {
    "id": "b9ab0b89"
   },
   "source": [
    "loading predictions to df: (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767eae12",
   "metadata": {
    "id": "767eae12"
   },
   "outputs": [],
   "source": [
    "df_chopsticks = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/df_chopsticks_pred.pkl')\n",
    "df_chopsticks_pos = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/df_chopsticks_pos.pkl')\n",
    "df_chopsticks_neg = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/df_chopsticks_neg.pkl')\n",
    "\n",
    "df_all = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/df_all_pred.pkl')\n",
    "df_all_pos = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/df_all_pos.pkl')\n",
    "df_all_neg = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/df_all_neg.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079debe",
   "metadata": {
    "id": "9079debe"
   },
   "source": [
    "manually sorting by week: (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d3f846",
   "metadata": {
    "id": "e6d3f846"
   },
   "outputs": [],
   "source": [
    "# aggregate data and get aggregated statistics\n",
    "pos_byweek_all = df_all_pos.groupby(by=['week_start_date']).agg({'retweet_count': ['sum']}).reset_index() # for positive tweets, get total retweets each week (sum of num retweets column over each week)\n",
    "pos_byweek_all.columns = ['date', 'total_retweets']\n",
    "\n",
    "neg_byweek_all = df_all_neg.groupby(by=['week_start_date']).agg({'retweet_count': ['sum']}).reset_index() # for negative tweets, get total retweets each week (sum of num retweets column over each week)\n",
    "neg_byweek_all.columns = ['date', 'total_retweets']\n",
    "\n",
    "# aggregate data and get aggregated statistics\n",
    "pos_byweek_chopsticks = df_chopsticks_pos.groupby(by=['week_start_date']).agg({'retweet_count': ['sum']}).reset_index() # for positive tweets, get total retweets each week (sum of num retweets column over each week)\n",
    "pos_byweek_chopsticks.columns = ['date', 'total_retweets']\n",
    "\n",
    "neg_byweek_chopsticks = df_chopsticks_neg.groupby(by=['week_start_date']).agg({'retweet_count': ['sum']}).reset_index() # for negative tweets, get total retweets each week (sum of num retweets column over each week)\n",
    "neg_byweek_chopsticks.columns = ['date', 'total_retweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66cd75",
   "metadata": {
    "id": "6c66cd75"
   },
   "outputs": [],
   "source": [
    "# for each week, get list of tweets with the highest number of retweets (which you can manually review to see what was trending that week)\n",
    "top_tweets_peak_weeks_all = df_all.sort_values(by='retweet_count', ascending=False)\n",
    "top_tweets_peak_weeks_all = top_tweets_peak_weeks.groupby('week_start_date').head(10).sort_values(by=['week_start_date', 'retweet_count'], ascending=False)\n",
    "top_tweets_peak_weeks_all\n",
    "#top_tweets_peak_weeks.to_csv(os.path.join(data_dir, 'top_tweets_peak_weeks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1982321",
   "metadata": {
    "id": "d1982321"
   },
   "outputs": [],
   "source": [
    "# for each week, get list of tweets with the highest number of retweets (which you can manually review to see what was trending that week)\n",
    "top_tweets_peak_weeks_chopsticks = df_chopsticks.sort_values(by='retweet_count', ascending=False)\n",
    "top_tweets_peak_weeks_chopsticks = top_tweets_peak_weeks_chopsticks.groupby('week_start_date').head(10).sort_values(by=['week_start_date', 'retweet_count'], ascending=False)\n",
    "top_tweets_peak_weeks_chopsticks\n",
    "#top_tweets_peak_weeks.to_csv(os.path.join(data_dir, 'top_tweets_peak_weeks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de51b4",
   "metadata": {
    "id": "c6de51b4"
   },
   "source": [
    "loading by week: (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84c254",
   "metadata": {
    "id": "3a84c254"
   },
   "outputs": [],
   "source": [
    "pos_byweek_chopsticks = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/pos_byweek_chopsticks.pkl')\n",
    "neg_byweek_chopsticks = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/neg_byweek_chopsticks.pkl')\n",
    "top_tweets_peak_weeks_chopsticks = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/top_tweets_peak_weeks_chopsticks.pkl')\n",
    "\n",
    "\n",
    "pos_byweek_all = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/pos_byweek_all.pkl')\n",
    "neg_byweek_all = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/neg_byweek_all.pkl')\n",
    "top_tweets_peak_weeks_all = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/top_tweets_peak_weeks_all.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51035f85",
   "metadata": {
    "id": "51035f85"
   },
   "source": [
    "manual tweet #: (5b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d9c7e",
   "metadata": {
    "id": "c79d9c7e",
    "outputId": "4014d798-9559-470d-d6b9-30f1fb0a6fe8"
   },
   "outputs": [],
   "source": [
    "countlist = []\n",
    "for i in range (len(df_all_pos)):\n",
    "    countlist.append(1)\n",
    "    \n",
    "df_all_pos['counting'] = countlist\n",
    "df_all_pos\n",
    "\n",
    "pos_byweek_tweets_all = df_all_pos.groupby(by=['week_start_date']).agg({'counting': ['sum']}).reset_index() \n",
    "pos_byweek_tweets_all.columns = ['date', '# of tweets']\n",
    "pos_byweek_tweets_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b4f19",
   "metadata": {
    "id": "0b8b4f19",
    "outputId": "d0d705dc-18c9-45d2-e6e7-29149b568b2e"
   },
   "outputs": [],
   "source": [
    "countlist = []\n",
    "for i in range (len(df_chopsticks_pos)):\n",
    "    countlist.append(1)\n",
    "    \n",
    "df_chopsticks_pos['counting'] = countlist\n",
    "df_chopsticks_pos\n",
    "\n",
    "pos_byweek_tweets_chopsticks = df_chopsticks_pos.groupby(by=['week_start_date']).agg({'counting': ['sum']}).reset_index() \n",
    "pos_byweek_tweets_chopsticks.columns = ['date', '# of tweets']\n",
    "pos_byweek_tweets_chopsticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b049f5",
   "metadata": {
    "id": "23b049f5",
    "outputId": "9ddcb369-335a-4b1b-e880-a44d41f6ba2c"
   },
   "outputs": [],
   "source": [
    "countlist = []\n",
    "for i in range (len(df_chopsticks_neg)):\n",
    "    countlist.append(1)\n",
    "    \n",
    "df_chopsticks_neg['counting'] = countlist\n",
    "df_chopsticks_neg\n",
    "\n",
    "neg_byweek_tweets_chopsticks = df_chopsticks_neg.groupby(by=['week_start_date']).agg({'counting': ['sum']}).reset_index() \n",
    "neg_byweek_tweets_chopsticks.columns = ['date', '# of tweets']\n",
    "neg_byweek_tweets_chopsticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261a010",
   "metadata": {
    "id": "0261a010",
    "outputId": "a4081ab9-2a95-4d8a-b884-3b9005e351e1"
   },
   "outputs": [],
   "source": [
    "countlist = []\n",
    "for i in range (len(df_all_neg)):\n",
    "    countlist.append(1)\n",
    "    \n",
    "df_all_neg['counting'] = countlist\n",
    "df_all_neg\n",
    "\n",
    "neg_byweek_tweets_all = df_all_neg.groupby(by=['week_start_date']).agg({'counting': ['sum']}).reset_index() \n",
    "neg_byweek_tweets_all.columns = ['date', '# of tweets']\n",
    "neg_byweek_tweets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fdfd21",
   "metadata": {
    "id": "b4fdfd21"
   },
   "outputs": [],
   "source": [
    "pickle.dump(neg_byweek_tweets_chopsticks, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/neg_byweek_tweets_chopsticks.pkl\", \"wb\")) \n",
    "pickle.dump(neg_byweek_tweets_all, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/neg_byweek_tweets_all.pkl\", \"wb\")) \n",
    "pickle.dump(pos_byweek_tweets_chopsticks, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/pos_byweek_tweets_chopsticks.pkl\", \"wb\")) \n",
    "pickle.dump(pos_byweek_tweets_all, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/pos_byweek_tweets_all.pkl\", \"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcca71b",
   "metadata": {
    "id": "1bcca71b"
   },
   "source": [
    "loading tweet #: (5b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b6485",
   "metadata": {
    "id": "f97b6485"
   },
   "outputs": [],
   "source": [
    "neg_byweek_tweets_chopsticks = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/neg_byweek_tweets_chopsticks.pkl\", 'rb'))\n",
    "neg_byweek_tweets_all = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/neg_byweek_tweets_all.pkl\", 'rb'))\n",
    "pos_byweek_tweets_chopsticks = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/pos_byweek_tweets_chopsticks.pkl\", 'rb'))\n",
    "pos_byweek_tweets_all = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/100k train/pos_byweek_tweets_all.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009c2b9",
   "metadata": {
    "id": "4009c2b9"
   },
   "source": [
    "manually making time series graphs: (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640fe67f",
   "metadata": {
    "id": "640fe67f"
   },
   "outputs": [],
   "source": [
    "# plot number of retweets over time (aggregated over each week) for positive and negative tweets\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "ax.plot(pos_byweek_all[\"date\"], pos_byweek_all['total_retweets'], c='blue', label='Positive tweets')\n",
    "ax.plot(neg_byweek_all[\"date\"], neg_byweek_all['total_retweets'], c='red', label='Negative tweets')\n",
    "\n",
    "\"\"\"\n",
    "# format the vertical lines and annotations\n",
    "y = {\n",
    "  0: 20000,\n",
    "  1: 40000,\n",
    "  2: 30000,\n",
    "  3: 60000,\n",
    "  4: 80000,\n",
    "  5: 50000,\n",
    "  6: 40000,\n",
    "  7: 40000,\n",
    "  8: 60000,\n",
    "  9: 80000,\n",
    "  10: 40000,\n",
    "  11: 140000,\n",
    "  12: 120000,\n",
    "  13: 100000,\n",
    "  14: 160000,\n",
    "  15: 160000,\n",
    "  16: 100000,\n",
    "  17: 120000,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "y_offset = 10000\n",
    "left_align = [1, 4, 5, 8, 9, 10, 15, 16, 17]\n",
    "\n",
    "\"\"\"\n",
    "# vertical line for major events\n",
    "for i in range(len(dates)):\n",
    "  date = dates[i]\n",
    "  event = events[i]\n",
    "  if i in left_align:\n",
    "    align = 'left'\n",
    "  else:\n",
    "    align = 'right'\n",
    "  plt.axvline(x=date, color='grey', linestyle=':')\n",
    "  ax.text(date, y[i]-y_offset, event, ha=align, size=10)\n",
    "\"\"\"\n",
    "\n",
    "# Major ticks every 6 months.\n",
    "fmt_half_year = mdates.MonthLocator(interval=6)\n",
    "ax.xaxis.set_major_locator(fmt_half_year)\n",
    "\n",
    "# Minor ticks every month.\n",
    "fmt_month = mdates.MonthLocator()\n",
    "ax.xaxis.set_minor_locator(fmt_month)\n",
    "\n",
    "# Text in the x axis will be displayed in 'YYYY-mm' format.\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Round to nearest half of year.\n",
    "datemin = np.datetime64(datetime.date(2013, 7, 1), 'm')\n",
    "datemax = np.datetime64(datetime.date(2021, 7, 1), 'm')\n",
    "ax.set_xlim(datemin, datemax)\n",
    "\n",
    "# Format the coords message box, i.e. the numbers displayed as the cursor moves\n",
    "# across the axes within the interactive GUI.\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.format_ydata = lambda x: f'${x:.2f}'  # Format the price.\n",
    "ax.grid(True)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Date', ylabel='Weekly total retweet count')\n",
    "ax.set_ylim([0, 10000]) # scale the y-axis range\n",
    "\n",
    "# Rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them.\n",
    "fig.autofmt_xdate()\n",
    "plt.legend() # add the legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939256c6",
   "metadata": {
    "id": "939256c6"
   },
   "outputs": [],
   "source": [
    "# plot number of retweets over time (aggregated over each week) for positive and negative tweets\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "ax.plot(neg_byweek_all[\"date\"], neg_byweek_all['total_retweets'], c='red', label='Negative tweets')\n",
    "\n",
    "\"\"\"\n",
    "# format the vertical lines and annotations\n",
    "y = {\n",
    "  0: 20000,\n",
    "  1: 40000,\n",
    "  2: 30000,\n",
    "  3: 60000,\n",
    "  4: 80000,\n",
    "  5: 50000,\n",
    "  6: 40000,\n",
    "  7: 40000,\n",
    "  8: 60000,\n",
    "  9: 80000,\n",
    "  10: 40000,\n",
    "  11: 140000,\n",
    "  12: 120000,\n",
    "  13: 100000,\n",
    "  14: 160000,\n",
    "  15: 160000,\n",
    "  16: 100000,\n",
    "  17: 120000,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "y_offset = 10000\n",
    "left_align = [1, 4, 5, 8, 9, 10, 15, 16, 17]\n",
    "\n",
    "\"\"\"\n",
    "# vertical line for major events\n",
    "for i in range(len(dates)):\n",
    "  date = dates[i]\n",
    "  event = events[i]\n",
    "  if i in left_align:\n",
    "    align = 'left'\n",
    "  else:\n",
    "    align = 'right'\n",
    "  plt.axvline(x=date, color='grey', linestyle=':')\n",
    "  ax.text(date, y[i]-y_offset, event, ha=align, size=10)\n",
    "\"\"\"\n",
    "\n",
    "# Major ticks every 6 months.\n",
    "fmt_half_year = mdates.MonthLocator(interval=6)\n",
    "ax.xaxis.set_major_locator(fmt_half_year)\n",
    "\n",
    "# Minor ticks every month.\n",
    "fmt_month = mdates.MonthLocator()\n",
    "ax.xaxis.set_minor_locator(fmt_month)\n",
    "\n",
    "# Text in the x axis will be displayed in 'YYYY-mm' format.\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Round to nearest half of year.\n",
    "datemin = np.datetime64(datetime.date(2013, 7, 1), 'm')\n",
    "datemax = np.datetime64(datetime.date(2021, 7, 1), 'm')\n",
    "ax.set_xlim(datemin, datemax)\n",
    "\n",
    "# Format the coords message box, i.e. the numbers displayed as the cursor moves\n",
    "# across the axes within the interactive GUI.\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.format_ydata = lambda x: f'${x:.2f}'  # Format the price.\n",
    "ax.grid(True)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Date', ylabel='Weekly total retweet count')\n",
    "ax.set_ylim([0, 1000]) # scale the y-axis range\n",
    "\n",
    "# Rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them.\n",
    "fig.autofmt_xdate()\n",
    "plt.legend() # add the legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb314e0",
   "metadata": {
    "id": "ccb314e0"
   },
   "source": [
    "samething but tweet #: (6b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5cd18",
   "metadata": {
    "id": "a4e5cd18",
    "outputId": "eedaffba-a990-488c-e797-0512045fc05f"
   },
   "outputs": [],
   "source": [
    "# plot number of retweets over time (aggregated over each week) for positive and negative tweets\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "ax.plot(pos_byweek_tweets_all[\"date\"], pos_byweek_tweets_all['# of tweets'], c='blue', label='Positive tweets')\n",
    "ax.plot(neg_byweek_tweets_all[\"date\"], neg_byweek_tweets_all['# of tweets'], c='red', label='Negative tweets')\n",
    "\n",
    "\"\"\"\n",
    "# format the vertical lines and annotations\n",
    "y = {\n",
    "  0: 20000,\n",
    "  1: 40000,\n",
    "  2: 30000,\n",
    "  3: 60000,\n",
    "  4: 80000,\n",
    "  5: 50000,\n",
    "  6: 40000,\n",
    "  7: 40000,\n",
    "  8: 60000,\n",
    "  9: 80000,\n",
    "  10: 40000,\n",
    "  11: 140000,\n",
    "  12: 120000,\n",
    "  13: 100000,\n",
    "  14: 160000,\n",
    "  15: 160000,\n",
    "  16: 100000,\n",
    "  17: 120000,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "y_offset = 10000\n",
    "left_align = [1, 4, 5, 8, 9, 10, 15, 16, 17]\n",
    "\n",
    "\"\"\"\n",
    "# vertical line for major events\n",
    "for i in range(len(dates)):\n",
    "  date = dates[i]\n",
    "  event = events[i]\n",
    "  if i in left_align:\n",
    "    align = 'left'\n",
    "  else:\n",
    "    align = 'right'\n",
    "  plt.axvline(x=date, color='grey', linestyle=':')\n",
    "  ax.text(date, y[i]-y_offset, event, ha=align, size=10)\n",
    "\"\"\"\n",
    "\n",
    "# Major ticks every 6 months.\n",
    "fmt_half_year = mdates.MonthLocator(interval=6)\n",
    "ax.xaxis.set_major_locator(fmt_half_year)\n",
    "\n",
    "# Minor ticks every month.\n",
    "fmt_month = mdates.MonthLocator()\n",
    "ax.xaxis.set_minor_locator(fmt_month)\n",
    "\n",
    "# Text in the x axis will be displayed in 'YYYY-mm' format.\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Round to nearest half of year.\n",
    "datemin = np.datetime64(datetime.date(2013, 7, 1), 'm')\n",
    "datemax = np.datetime64(datetime.date(2021, 7, 1), 'm')\n",
    "ax.set_xlim(datemin, datemax)\n",
    "\n",
    "# Format the coords message box, i.e. the numbers displayed as the cursor moves\n",
    "# across the axes within the interactive GUI.\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.format_ydata = lambda x: f'${x:.2f}'  # Format the price.\n",
    "ax.grid(True)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Date', ylabel='Weekly total retweet count')\n",
    "ax.set_ylim([0, 2000]) # scale the y-axis range\n",
    "\n",
    "# Rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them.\n",
    "fig.autofmt_xdate()\n",
    "plt.legend() # add the legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289240cd",
   "metadata": {
    "id": "289240cd",
    "outputId": "d717dad1-c396-422f-ad42-840160c87300",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot number of retweets over time (aggregated over each week) for positive and negative tweets\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "#ax.plot(pos_byweek_tweets_chopsticks[\"date\"], pos_byweek_tweets_chopsticks['# of tweets'], c='blue', label='Positive tweets')\n",
    "ax.plot(neg_byweek_tweets_all[\"date\"], neg_byweek_tweets_all['# of tweets'], c='red', label='Negative tweets')\n",
    "\n",
    "\"\"\"\n",
    "# format the vertical lines and annotations\n",
    "y = {\n",
    "  0: 20000,\n",
    "  1: 40000,\n",
    "  2: 30000,\n",
    "  3: 60000,\n",
    "  4: 80000,\n",
    "  5: 50000,\n",
    "  6: 40000,\n",
    "  7: 40000,\n",
    "  8: 60000,\n",
    "  9: 80000,\n",
    "  10: 40000,\n",
    "  11: 140000,\n",
    "  12: 120000,\n",
    "  13: 100000,\n",
    "  14: 160000,\n",
    "  15: 160000,\n",
    "  16: 100000,\n",
    "  17: 120000,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "y_offset = 10000\n",
    "left_align = [1, 4, 5, 8, 9, 10, 15, 16, 17]\n",
    "\n",
    "\"\"\"\n",
    "# vertical line for major events\n",
    "for i in range(len(dates)):\n",
    "  date = dates[i]\n",
    "  event = events[i]\n",
    "  if i in left_align:\n",
    "    align = 'left'\n",
    "  else:\n",
    "    align = 'right'\n",
    "  plt.axvline(x=date, color='grey', linestyle=':')\n",
    "  ax.text(date, y[i]-y_offset, event, ha=align, size=10)\n",
    "\"\"\"\n",
    "\n",
    "# Major ticks every 6 months.\n",
    "fmt_half_year = mdates.MonthLocator(interval=6)\n",
    "ax.xaxis.set_major_locator(fmt_half_year)\n",
    "\n",
    "# Minor ticks every month.\n",
    "fmt_month = mdates.MonthLocator()\n",
    "ax.xaxis.set_minor_locator(fmt_month)\n",
    "\n",
    "# Text in the x axis will be displayed in 'YYYY-mm' format.\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Round to nearest half of year.\n",
    "datemin = np.datetime64(datetime.date(2013, 7, 1), 'm')\n",
    "datemax = np.datetime64(datetime.date(2021, 7, 1), 'm')\n",
    "ax.set_xlim(datemin, datemax)\n",
    "\n",
    "# Format the coords message box, i.e. the numbers displayed as the cursor moves\n",
    "# across the axes within the interactive GUI.\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.format_ydata = lambda x: f'${x:.2f}'  # Format the price.\n",
    "ax.grid(True)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Date', ylabel='Weekly total retweet count')\n",
    "ax.set_ylim([0, 250]) # scale the y-axis range\n",
    "\n",
    "# Rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them.\n",
    "fig.autofmt_xdate()\n",
    "plt.legend() # add the legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d7a166",
   "metadata": {
    "id": "b4d7a166"
   },
   "source": [
    "## RFC\n",
    "manually tokenizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd57de",
   "metadata": {
    "id": "94bd57de"
   },
   "outputs": [],
   "source": [
    "model = CountVectorizer(preprocessor=preprocessor, tokenizer=tokenizer, max_features=5000)\n",
    "word_counts_train = model.fit_transform(corpus_train)\n",
    "tokens_train = model.get_feature_names()\n",
    "fitted_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99b2f8",
   "metadata": {
    "id": "fe99b2f8"
   },
   "outputs": [],
   "source": [
    "pickle.dump(fitted_model, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/fitted_model.pkl\", \"wb\")) \n",
    "pickle.dump(model_features, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/model_features\", 'wb'))\n",
    "\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/word_counts_train.npz', word_counts_train, compressed=True)\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/word_counts_chopsticks.npz', word_counts_chopsticks, compressed=True)\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/word_counts_test.npz', word_counts_test, compressed=True)\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/word_counts_all.npz', word_counts_all, compressed=True)\n",
    "\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_140.npz', X_140, compressed=True)\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_test.npz', X_test, compressed=True)\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_chopsticks.npz', X_chopsticks, compressed=True)\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_all.npz', X_all, compressed=True)\n",
    "\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_train.npz', X_train, compressed=True)\n",
    "sparse.save_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_validation.npz', X_validation, compressed=True)\n",
    "pickle.dump(y_trainsmall, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_trainsmall\", 'wb'))\n",
    "pickle.dump(y_validation, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_validation\", 'wb'))\n",
    "pickle.dump(y_test, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_test\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f49ad",
   "metadata": {
    "id": "e62f49ad"
   },
   "outputs": [],
   "source": [
    "model_features = model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390735c5",
   "metadata": {
    "id": "390735c5"
   },
   "outputs": [],
   "source": [
    "word_counts_test = fitted_model.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71122efa",
   "metadata": {
    "id": "71122efa"
   },
   "outputs": [],
   "source": [
    "word_counts_chopsticks = fitted_model.transform(corpus_chopsticks)\n",
    "word_counts_all = fitted_model.transform(corpus_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cddac0",
   "metadata": {
    "id": "96cddac0",
    "outputId": "cc782ca5-f3b9-4926-cfa5-c8e25041d2ea"
   },
   "outputs": [],
   "source": [
    "fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e9baa",
   "metadata": {
    "id": "e82e9baa"
   },
   "outputs": [],
   "source": [
    "# use TF-IDF transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_140 = tfidf_transformer.fit_transform(word_counts_train) # training\n",
    "X_test = tfidf_transformer.transform(word_counts_test) # testing\n",
    "X_chopsticks = tfidf_transformer.transform(word_counts_chopsticks) # training\n",
    "X_all =  tfidf_transformer.transform(word_counts_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda35b2",
   "metadata": {
    "id": "8fda35b2",
    "outputId": "af851989-e79c-4746-8f17-d0f24678b529"
   },
   "outputs": [],
   "source": [
    "X_140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f76a1",
   "metadata": {
    "id": "447f76a1"
   },
   "outputs": [],
   "source": [
    "X_train, X_validation, y_trainsmall, y_validation = train_test_split(X_140, y_train, test_size=0.10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb0bc8",
   "metadata": {
    "id": "dbdb0bc8",
    "outputId": "8565b0d0-d553-4157-bb09-46b79f850aad"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211aff96",
   "metadata": {
    "id": "211aff96"
   },
   "source": [
    "loading tokenized data: (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f98b5",
   "metadata": {
    "id": "e12f98b5"
   },
   "outputs": [],
   "source": [
    "#loading vectorizer path\n",
    "fitted_model = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/fitted_model.pkl\", 'rb'))\n",
    "model_features = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/model_features', allow_pickle=True)\n",
    "\n",
    "word_counts_test = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/word_counts_test.npz')\n",
    "word_counts_chopsticks = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/word_counts_chopsticks.npz')\n",
    "word_counts_all = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/word_counts_all.npz')\n",
    "word_counts_train = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/word_counts_train.npz')\n",
    "\n",
    "X_140 = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_140.npz')\n",
    "X_test = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_test.npz')\n",
    "X_chopsticks = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_chopsticks.npz')\n",
    "X_all = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_all.npz')\n",
    "\n",
    "X_train = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_train.npz')\n",
    "X_validation = sparse.load_npz('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/X_validation.npz')\n",
    "y_trainsmall = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_trainsmall', allow_pickle=True)\n",
    "y_validation = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_validation', allow_pickle=True)\n",
    "y_test = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_test', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03daa9",
   "metadata": {
    "id": "cc03daa9"
   },
   "source": [
    "manual rfc: (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170dd66",
   "metadata": {
    "id": "2170dd66",
    "outputId": "86271c17-decb-458f-a96b-8fe6173c3154"
   },
   "outputs": [],
   "source": [
    "rfc_model = RandomForestClassifier(min_samples_split=90)\n",
    "#rfc_model = RandomForestClassifier(max_depth=10, min_samples_split=90, max_leaf_nodes=8, min_samples_leaf=7, n_estimators=110, max_samples=0.65, max_features=80, random_state=0)\n",
    "rfc_model.fit(X_train, y_trainsmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916f17c",
   "metadata": {
    "id": "7916f17c"
   },
   "outputs": [],
   "source": [
    "# prediction on training\n",
    "y_train_pred_rfc = rfc_model.predict(X_train)\n",
    "y_train_proba_rfc = rfc_model.predict_proba(X_train)\n",
    "\n",
    "# prediction on validation\n",
    "y_validation_pred_rfc = rfc_model.predict(X_validation)\n",
    "y_validation_proba_rfc = rfc_model.predict_proba(X_validation)\n",
    "\n",
    "# prediction on chopsticks\n",
    "y_chopsticks_pred_rfc = rfc_model.predict(X_chopsticks)\n",
    "y_chopsticks_proba_rfc = rfc_model.predict_proba(X_chopsticks)\n",
    "\n",
    "# prediction on d&g overall\n",
    "y_all_pred_rfc = rfc_model.predict(X_all)\n",
    "y_all_proba_rfc = rfc_model.predict_proba(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33747e12",
   "metadata": {
    "id": "33747e12",
    "outputId": "0267e176-aa71-40ff-82de-a10ab1daf46e"
   },
   "outputs": [],
   "source": [
    "print('Training performance')\n",
    "print(classification_report(y_trainsmall, y_train_pred_svc))\n",
    "print()\n",
    "print('====================')\n",
    "print()\n",
    "print('Validation performance')\n",
    "print(classification_report(y_validation, y_validation_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53c3ec",
   "metadata": {
    "id": "bb53c3ec",
    "outputId": "e6f402ce-cff0-4093-d6e2-4baaa260a030"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(y_validation_proba_rfc[:,1])\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.title('Predicted probability distirbution training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29477203",
   "metadata": {
    "id": "29477203"
   },
   "outputs": [],
   "source": [
    "pickle.dump(rfc_model, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/rfc_model.pkl\", \"wb\")) \n",
    "\n",
    "pickle.dump(y_train_pred_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_train_pred_rfc\", 'wb'))\n",
    "pickle.dump(y_train_proba_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_train_proba_rfc\", 'wb'))\n",
    "pickle.dump(y_validation_pred_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_validation_pred_rfc\", 'wb'))\n",
    "pickle.dump(y_validation_proba_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_validation_proba_rfc\", 'wb'))\n",
    "pickle.dump(y_chopsticks_pred_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_chopsticks_pred_rfc_binary\", 'wb'))\n",
    "pickle.dump(y_chopsticks_proba_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_chopsticks_proba_rfc\", 'wb'))\n",
    "pickle.dump(y_all_pred_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_all_pred_rfc_binary\", 'wb'))\n",
    "pickle.dump(y_all_proba_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_all_proba_rfc\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef938fe",
   "metadata": {
    "id": "9ef938fe"
   },
   "source": [
    "loading rfc: (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceeb764",
   "metadata": {
    "id": "bceeb764"
   },
   "outputs": [],
   "source": [
    "rfc_model = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/rfc_model.pkl\", 'rb'))\n",
    "\n",
    "y_train_pred_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_train_pred_rfc', allow_pickle=True)\n",
    "y_train_proba_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_train_proba_rfc', allow_pickle=True)\n",
    "y_validation_pred_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_validation_pred_rfc', allow_pickle=True)\n",
    "y_validation_proba_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_validation_proba_rfc', allow_pickle=True)\n",
    "y_chopsticks_pred_rfc_binary = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_chopsticks_pred_rfc_binary', allow_pickle=True)\n",
    "y_chopsticks_proba_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_chopsticks_proba_rfc', allow_pickle=True)\n",
    "y_all_pred_rfc_binary = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_all_pred_rfc_binary', allow_pickle=True)\n",
    "y_all_proba_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_all_proba_rfc', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d659c7",
   "metadata": {
    "id": "40d659c7"
   },
   "source": [
    "manual 140test/probability (3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafda5aa",
   "metadata": {
    "id": "eafda5aa"
   },
   "outputs": [],
   "source": [
    "# prediction on testing (once hyperparameters are tuned)\n",
    "y_test_pred_rfc_binary = rfc_model.predict(X_test)\n",
    "y_test_proba_rfc = rfc_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83508b9c",
   "metadata": {
    "id": "83508b9c",
    "outputId": "0df76765-ab27-458f-cd94-4c819eb8a27f"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(y_chopsticks_proba_rfc[:,1])\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.title('Predicted probability distribution validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1af8f",
   "metadata": {
    "id": "48e1af8f",
    "outputId": "3e705b62-7c9a-4e2a-d42f-d1726eac75cf"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(y_all_proba_svc[:,1])\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.title('Predicted probability distribution validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ee460",
   "metadata": {
    "id": "144ee460",
    "outputId": "db5dd383-b840-4acb-a694-7606b22123c5"
   },
   "outputs": [],
   "source": [
    "y_chopsticks_proba_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb03c8a",
   "metadata": {
    "id": "ddb03c8a",
    "outputId": "0a385125-0c26-4d81-bb41-6ef455263a79"
   },
   "outputs": [],
   "source": [
    "topacc = 0\n",
    "top_a = 0\n",
    "top_b = 0\n",
    "for i in range (10):\n",
    "    a = 0.48+(i*0.01)\n",
    "    for i in range (10):\n",
    "        y_test_pred_rfc_binary = []\n",
    "        b = 0.83-(i*0.01)\n",
    "        for i in range (len(y_test_proba_rfc)):\n",
    "            if (y_test_proba_rfc[i,1])<a:\n",
    "                y_test_pred_rfc_binary.append(0)\n",
    "            elif(y_test_proba_rfc[i,1])>b:\n",
    "                y_test_pred_rfc_binary.append(4)\n",
    "            else:\n",
    "                y_test_pred_rfc_binary.append(2)\n",
    "        accuracy = accuracy_score(y_test, y_test_pred_rfc_binary)\n",
    "        if accuracy > topacc:\n",
    "            topacc = accuracy\n",
    "            top_a = a\n",
    "            top_b = b\n",
    "            top_y_pred = y_test_pred_rfc_binary\n",
    "        print(\"acc = \" + str(accuracy)+ \" a = \"+str(a)+\" b = \"+str(b))\n",
    "            \n",
    "        \n",
    "print(\"overall top acc = \"+str(topacc)+ \" a = \"+str(top_a)+\" b = \"+str(top_b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a0333",
   "metadata": {
    "id": "6c3a0333"
   },
   "outputs": [],
   "source": [
    "#best cutoff values: a = 0.48 b = 0.83 (positive proba(second column))\n",
    "y_test_pred_rfc = top_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73768bd",
   "metadata": {
    "id": "d73768bd",
    "outputId": "750d8e4e-09ce-4ad4-936a-2f65868ee7cf"
   },
   "outputs": [],
   "source": [
    "print('Testing performance')\n",
    "print(classification_report(y_test, y_test_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f471ff",
   "metadata": {
    "id": "f2f471ff"
   },
   "outputs": [],
   "source": [
    "y_chopsticks_pred_rfc = []\n",
    "for i in range (len(y_chopsticks_proba_rfc)):\n",
    "            if (y_chopsticks_proba_rfc[i,1])<0.5:\n",
    "                y_chopsticks_pred_rfc.append(0)\n",
    "            elif(y_chopsticks_proba_rfc[i,1])>0.69:\n",
    "                y_chopsticks_pred_rfc.append(4)\n",
    "            else:\n",
    "                y_chopsticks_pred_rfc.append(2)\n",
    "y_all_pred_rfc = []\n",
    "for i in range (len(y_all_proba_rfc)):\n",
    "            if (y_all_proba_rfc[i,1])<0.5:\n",
    "                y_all_pred_rfc.append(0)\n",
    "            elif(y_all_proba_rfc[i,1])>0.69:\n",
    "                y_all_pred_rfc.append(4)\n",
    "            else:\n",
    "                y_all_pred_rfc.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e6b64",
   "metadata": {
    "id": "b48e6b64",
    "outputId": "dade8f25-5a34-4d6d-8f2b-4db04650a213"
   },
   "outputs": [],
   "source": [
    "# see where model predicted wrong\n",
    "for i in range(len(y_test_pred_rfc)):\n",
    "  if y_test[i] != y_test_pred_rfc[i]:\n",
    "    tweet = corpus_test[i]\n",
    "    print(f'y_true {y_test[i]}, y_pred {y_test_pred_rfc[i]}')\n",
    "    print(tweet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c2dc7",
   "metadata": {
    "id": "f91c2dc7"
   },
   "outputs": [],
   "source": [
    "pickle.dump(y_test_proba_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_test_proba_rfc\", 'wb'))\n",
    "pickle.dump(y_test_pred_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_test_pred_rfc\", 'wb'))\n",
    "pickle.dump(y_chopsticks_pred_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_chopsticks_pred_rfc\", 'wb'))\n",
    "pickle.dump(y_all_pred_rfc, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_all_pred_rfc\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e16387",
   "metadata": {
    "id": "02e16387"
   },
   "source": [
    "loading probability: (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62d14f",
   "metadata": {
    "id": "9f62d14f"
   },
   "outputs": [],
   "source": [
    "y_test_proba_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_test_proba_rfc', allow_pickle=True)\n",
    "y_test_pred_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_test_pred_rfc', allow_pickle=True)\n",
    "y_chopsticks_pred_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_chopsticks_pred_rfc', allow_pickle=True)\n",
    "y_all_pred_rfc = np.load('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/y_all_pred_rfc', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd62c52",
   "metadata": {
    "id": "ecd62c52"
   },
   "source": [
    "manual adding predictions: (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975149b",
   "metadata": {
    "id": "2975149b",
    "outputId": "29458ac6-fd54-4b45-bc35-6e6ada55a978"
   },
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e55a6a",
   "metadata": {
    "id": "54e55a6a",
    "outputId": "620ac280-7b24-4331-d70e-9e770a91e0f5"
   },
   "outputs": [],
   "source": [
    "df_chopsticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8a7b6",
   "metadata": {
    "id": "c7f8a7b6",
    "outputId": "c0ac1314-006e-4094-c823-3ee35cdab951"
   },
   "outputs": [],
   "source": [
    "y_chopsticks_proba_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f22bea",
   "metadata": {
    "id": "71f22bea"
   },
   "outputs": [],
   "source": [
    "df_chopsticks['pred_label'] = y_chopsticks_pred_rfc\n",
    "df_all['pred_label'] = y_all_pred_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f232c1e",
   "metadata": {
    "id": "7f232c1e"
   },
   "outputs": [],
   "source": [
    "df_chopsticks['pred_score'] = y_chopsticks_proba_rfc [:,1]\n",
    "df_all[ 'pred_score'] = y_all_proba_rfc [:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0446761",
   "metadata": {
    "id": "f0446761",
    "outputId": "6d63270a-a450-43c4-d9f6-44d295c085f9"
   },
   "outputs": [],
   "source": [
    "# Changing object type column to datetime\n",
    "df_chopsticks['interval_start_date'] = pd.to_datetime(df_chopsticks['interval_start_date'])\n",
    "df_chopsticks['created_at'] = pd.to_datetime(df_chopsticks['created_at'])\n",
    "df_chopsticks['date'] = df_chopsticks['created_at'].dt.date # Creating new column with just the date\n",
    "df_chopsticks['date'] = pd.to_datetime(df_chopsticks['date'])\n",
    "\n",
    "# get week start date to aggregate over weeks\n",
    "df_chopsticks['week_start_date'] = df_chopsticks['date'].apply(lambda x: x - datetime.timedelta(days=x.weekday()))\n",
    "df_chopsticks['week_start_date'] = df_chopsticks['week_start_date'].dt.date\n",
    "df_chopsticks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8a5f5",
   "metadata": {
    "id": "ecb8a5f5",
    "outputId": "c8798d01-2fa2-4c41-cec7-f2ae5ad47833"
   },
   "outputs": [],
   "source": [
    "# Changing object type column to datetime\n",
    "df_all['interval_start_date'] = pd.to_datetime(df_all['interval_start_date'])\n",
    "df_all['created_at'] = pd.to_datetime(df_all['created_at'])\n",
    "df_all['date'] = df_all['created_at'].dt.date # Creating new column with just the date\n",
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "\n",
    "# get week start date to aggregate over weeks\n",
    "df_all['week_start_date'] = df_all['date'].apply(lambda x: x - datetime.timedelta(days=x.weekday()))\n",
    "df_all['week_start_date'] = df_all['week_start_date'].dt.date\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414f14d",
   "metadata": {
    "id": "a414f14d"
   },
   "outputs": [],
   "source": [
    "# positive is going to be defined as greater than 80% prediction score that it is positive (adjustable based on threshold)\n",
    "df_all_pos = df_all[(df_all['pred_label'] == 4)&(df_all['pred_score']>0.8)]\n",
    "\n",
    "# negative is going to be defined as greater than 80% prediction score that it is negative (adjustable based on threshold)\n",
    "df_all_neg = df_all[(df_all['pred_label'] == 0)&(df_all['pred_score']<0.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490346d6",
   "metadata": {
    "id": "490346d6"
   },
   "outputs": [],
   "source": [
    "# positive is going to be defined as greater than 80% prediction score that it is positive (adjustable based on threshold)\n",
    "df_chopsticks_pos = df_chopsticks[(df_chopsticks['pred_label'] == 4)&(df_chopsticks['pred_score']>0.9)]\n",
    "\n",
    "# negative is going to be defined as greater than 80% prediction score that it is negative (adjustable based on threshold)\n",
    "df_chopsticks_neg = df_chopsticks[(df_chopsticks['pred_label'] == 0)&(df_chopsticks['pred_score']<0.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb6af1",
   "metadata": {
    "id": "90fb6af1",
    "outputId": "884e3aee-c17b-4425-ca55-0cb122a6ec11"
   },
   "outputs": [],
   "source": [
    "df_chopsticks_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35f02d",
   "metadata": {
    "id": "ae35f02d"
   },
   "outputs": [],
   "source": [
    "pickle.dump(df_chopsticks, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_chopsticks.pkl\", \"wb\")) \n",
    "pickle.dump(df_chopsticks_pos, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_chopsticks_pos.pkl\", \"wb\")) \n",
    "pickle.dump(df_chopsticks_neg, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_chopsticks_neg.pkl\", \"wb\")) \n",
    "\n",
    "pickle.dump(df_all, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_all.pkl\", \"wb\")) \n",
    "pickle.dump(df_all_pos, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_all_pos.pkl\", \"wb\")) \n",
    "pickle.dump(df_all_neg, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_all_neg.pkl\", \"wb\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d4bf2",
   "metadata": {
    "id": "501d4bf2"
   },
   "source": [
    "loading predictions: (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867590b",
   "metadata": {
    "id": "1867590b"
   },
   "outputs": [],
   "source": [
    "df_chopsticks = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_chopsticks.pkl')\n",
    "df_chopsticks_pos = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_chopsticks_pos.pkl')\n",
    "df_chopsticks_neg = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_chopsticks_neg.pkl')\n",
    "\n",
    "df_all = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_all.pkl')\n",
    "df_all_pos = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_all_pos.pkl')\n",
    "df_all_neg = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/df_all_neg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f86e61",
   "metadata": {
    "id": "23f86e61",
    "outputId": "b05d8101-d6e9-4b7a-ac72-04fb2fb27328",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_chopsticks_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d73aa",
   "metadata": {
    "id": "d26d73aa"
   },
   "source": [
    "manually sorting by week (5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354ec33",
   "metadata": {
    "id": "3354ec33"
   },
   "outputs": [],
   "source": [
    "# aggregate data and get aggregated statistics\n",
    "pos_byweek_all = df_all_pos.groupby(by=['week_start_date']).agg({'retweet_count': ['sum']}).reset_index() # for positive tweets, get total retweets each week (sum of num retweets column over each week)\n",
    "pos_byweek_all.columns = ['date', 'total_retweets']\n",
    "\n",
    "neg_byweek_all = df_all_neg.groupby(by=['week_start_date']).agg({'retweet_count': ['sum']}).reset_index() # for negative tweets, get total retweets each week (sum of num retweets column over each week)\n",
    "neg_byweek_all.columns = ['date', 'total_retweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21470545",
   "metadata": {
    "id": "21470545"
   },
   "outputs": [],
   "source": [
    "# aggregate data and get aggregated statistics\n",
    "pos_byweek_chopsticks = df_chopsticks_pos.groupby(by=['week_start_date']).agg({'retweet_count': ['sum']}).reset_index() # for positive tweets, get total retweets each week (sum of num retweets column over each week)\n",
    "pos_byweek_chopsticks.columns = ['date', 'total_retweets']\n",
    "\n",
    "neg_byweek_chopsticks = df_chopsticks_neg.groupby(by=['week_start_date']).agg({'retweet_count': ['sum']}).reset_index() # for negative tweets, get total retweets each week (sum of num retweets column over each week)\n",
    "neg_byweek_chopsticks.columns = ['date', 'total_retweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21c92b",
   "metadata": {
    "id": "dc21c92b",
    "outputId": "940ec155-d12c-42ec-bf44-78e74688f015"
   },
   "outputs": [],
   "source": [
    "# for each week, get list of tweets with the highest number of retweets (which you can manually review to see what was trending that week)\n",
    "top_tweets_peak_weeks_all = df_all.sort_values(by='retweet_count', ascending=False)\n",
    "top_tweets_peak_weeks_all = top_tweets_peak_weeks_all.groupby('week_start_date').head(10).sort_values(by=['week_start_date', 'retweet_count'], ascending=False)\n",
    "top_tweets_peak_weeks_all\n",
    "#top_tweets_peak_weeks.to_csv(os.path.join(data_dir, 'top_tweets_peak_weeks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a126e",
   "metadata": {
    "id": "959a126e",
    "outputId": "4aea9143-1d98-40a3-ad55-ad776dfbc52b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each week, get list of tweets with the highest number of retweets (which you can manually review to see what was trending that week)\n",
    "top_tweets_peak_weeks_chopsticks = df_chopsticks.sort_values(by='retweet_count', ascending=False)\n",
    "top_tweets_peak_weeks_chopsticks = top_tweets_peak_weeks_chopsticks.groupby('week_start_date').head(10).sort_values(by=['week_start_date', 'retweet_count'], ascending=False)\n",
    "top_tweets_peak_weeks_chopsticks\n",
    "#top_tweets_peak_weeks.to_csv(os.path.join(data_dir, 'top_tweets_peak_weeks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54818b56",
   "metadata": {
    "id": "54818b56"
   },
   "outputs": [],
   "source": [
    "pickle.dump(pos_byweek_chopsticks, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/pos_byweek_chopsticks.pkl\", \"wb\")) \n",
    "pickle.dump(neg_byweek_chopsticks, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/neg_byweek_chopsticks.pkl\", \"wb\")) \n",
    "pickle.dump(top_tweets_peak_weeks_chopsticks, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/top_tweets_peak_weeks_chopsticks.pkl\", \"wb\")) \n",
    "\n",
    "pickle.dump(pos_byweek_all, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/pos_byweek_all.pkl\", \"wb\")) \n",
    "pickle.dump(neg_byweek_all, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/neg_byweek_all.pkl\", \"wb\")) \n",
    "pickle.dump(top_tweets_peak_weeks_all, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/top_tweets_peak_weeks_all.pkl\", \"wb\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba7bda",
   "metadata": {
    "id": "09ba7bda"
   },
   "source": [
    "loading by week: (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338c646",
   "metadata": {
    "id": "a338c646"
   },
   "outputs": [],
   "source": [
    "pos_byweek_chopsticks = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/pos_byweek_chopsticks.pkl')\n",
    "neg_byweek_chopsticks = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/neg_byweek_chopsticks.pkl')\n",
    "top_tweets_peak_weeks_chopsticks = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/top_tweets_peak_weeks_chopsticks.pkl')\n",
    "\n",
    "\n",
    "pos_byweek_all = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/pos_byweek_all.pkl')\n",
    "neg_byweek_all = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/neg_byweek_all.pkl')\n",
    "top_tweets_peak_weeks_all = pd.read_pickle('C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/top_tweets_peak_weeks_all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81615855",
   "metadata": {
    "id": "81615855"
   },
   "source": [
    "manual tweet #: (5b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e49a9",
   "metadata": {
    "id": "9a2e49a9",
    "outputId": "59856cca-962b-46e4-b30d-49a45c9d62bc"
   },
   "outputs": [],
   "source": [
    "countlist = []\n",
    "for i in range (len(df_all_pos)):\n",
    "    countlist.append(1)\n",
    "    \n",
    "df_all_pos['counting'] = countlist\n",
    "df_all_pos\n",
    "\n",
    "pos_byweek_tweets_all = df_all_pos.groupby(by=['week_start_date']).agg({'counting': ['sum']}).reset_index() \n",
    "pos_byweek_tweets_all.columns = ['date', '# of tweets']\n",
    "pos_byweek_tweets_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a28982",
   "metadata": {
    "id": "34a28982",
    "outputId": "0c2c4939-b380-418b-bb6a-8e0293f7c170"
   },
   "outputs": [],
   "source": [
    "countlist = []\n",
    "for i in range (len(df_chopsticks_pos)):\n",
    "    countlist.append(1)\n",
    "    \n",
    "df_chopsticks_pos['counting'] = countlist\n",
    "df_chopsticks_pos\n",
    "\n",
    "pos_byweek_tweets_chopsticks = df_chopsticks_pos.groupby(by=['week_start_date']).agg({'counting': ['sum']}).reset_index() \n",
    "pos_byweek_tweets_chopsticks.columns = ['date', '# of tweets']\n",
    "pos_byweek_tweets_chopsticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6b36a",
   "metadata": {
    "id": "c1f6b36a",
    "outputId": "7d16bf1b-36e9-4cb8-ceb0-a54bc7c834a4"
   },
   "outputs": [],
   "source": [
    "countlist = []\n",
    "for i in range (len(df_chopsticks_neg)):\n",
    "    countlist.append(1)\n",
    "    \n",
    "df_chopsticks_neg['counting'] = countlist\n",
    "df_chopsticks_neg\n",
    "\n",
    "neg_byweek_tweets_chopsticks = df_chopsticks_neg.groupby(by=['week_start_date']).agg({'counting': ['sum']}).reset_index() \n",
    "neg_byweek_tweets_chopsticks.columns = ['date', '# of tweets']\n",
    "neg_byweek_tweets_chopsticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5169f81",
   "metadata": {
    "id": "c5169f81",
    "outputId": "b1d67912-d9d2-461b-b29a-0f338d5782ea"
   },
   "outputs": [],
   "source": [
    "countlist = []\n",
    "for i in range (len(df_all_neg)):\n",
    "    countlist.append(1)\n",
    "    \n",
    "df_all_neg['counting'] = countlist\n",
    "df_all_neg\n",
    "\n",
    "neg_byweek_tweets_all = df_all_neg.groupby(by=['week_start_date']).agg({'counting': ['sum']}).reset_index() \n",
    "neg_byweek_tweets_all.columns = ['date', '# of tweets']\n",
    "neg_byweek_tweets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6c852",
   "metadata": {
    "id": "36f6c852"
   },
   "outputs": [],
   "source": [
    "pickle.dump(neg_byweek_tweets_chopsticks, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/neg_byweek_tweets_chopsticks.pkl\", \"wb\")) \n",
    "pickle.dump(neg_byweek_tweets_all, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/neg_byweek_tweets_all.pkl\", \"wb\")) \n",
    "pickle.dump(pos_byweek_tweets_chopsticks, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/pos_byweek_tweets_chopsticks.pkl\", \"wb\")) \n",
    "pickle.dump(pos_byweek_tweets_all, open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/pos_byweek_tweets_all.pkl\", \"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1da0b6",
   "metadata": {
    "id": "5d1da0b6"
   },
   "source": [
    "loading tweet #: (5b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c7f6f",
   "metadata": {
    "id": "f35c7f6f"
   },
   "outputs": [],
   "source": [
    "neg_byweek_tweets_chopsticks = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/neg_byweek_tweets_chopsticks.pkl\", 'rb'))\n",
    "neg_byweek_tweets_all = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/neg_byweek_tweets_all.pkl\", 'rb'))\n",
    "pos_byweek_tweets_chopsticks = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/pos_byweek_tweets_chopsticks.pkl\", 'rb'))\n",
    "pos_byweek_tweets_all = pickle.load(open(\"C:/Users/flore/Desktop/UTS S5/STEM fellowship/rfc/pos_byweek_tweets_all.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082e5cd",
   "metadata": {
    "id": "f082e5cd"
   },
   "source": [
    "manual graph making: (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4f721",
   "metadata": {
    "id": "31b4f721",
    "outputId": "3f799035-fc13-4d97-a0cf-ab02f0198f72",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot number of retweets over time (aggregated over each week) for positive and negative tweets\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "ax.plot(neg_byweek_all[\"date\"], neg_byweek_all['total_retweets'], c='red', label='Negative tweets')\n",
    "\n",
    "\"\"\"\n",
    "# format the vertical lines and annotations\n",
    "y = {\n",
    "  0: 20000,\n",
    "  1: 40000,\n",
    "  2: 30000,\n",
    "  3: 60000,\n",
    "  4: 80000,\n",
    "  5: 50000,\n",
    "  6: 40000,\n",
    "  7: 40000,\n",
    "  8: 60000,\n",
    "  9: 80000,\n",
    "  10: 40000,\n",
    "  11: 140000,\n",
    "  12: 120000,\n",
    "  13: 100000,\n",
    "  14: 160000,\n",
    "  15: 160000,\n",
    "  16: 100000,\n",
    "  17: 120000,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "y_offset = 10000\n",
    "left_align = [1, 4, 5, 8, 9, 10, 15, 16, 17]\n",
    "\n",
    "\"\"\"\n",
    "# vertical line for major events\n",
    "for i in range(len(dates)):\n",
    "  date = dates[i]\n",
    "  event = events[i]\n",
    "  if i in left_align:\n",
    "    align = 'left'\n",
    "  else:\n",
    "    align = 'right'\n",
    "  plt.axvline(x=date, color='grey', linestyle=':')\n",
    "  ax.text(date, y[i]-y_offset, event, ha=align, size=10)\n",
    "\"\"\"\n",
    "\n",
    "# Major ticks every 6 months.\n",
    "fmt_half_year = mdates.MonthLocator(interval=6)\n",
    "ax.xaxis.set_major_locator(fmt_half_year)\n",
    "\n",
    "# Minor ticks every month.\n",
    "fmt_month = mdates.MonthLocator()\n",
    "ax.xaxis.set_minor_locator(fmt_month)\n",
    "\n",
    "# Text in the x axis will be displayed in 'YYYY-mm' format.\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Round to nearest half of year.\n",
    "datemin = np.datetime64(datetime.date(2013, 7, 1), 'm')\n",
    "datemax = np.datetime64(datetime.date(2021, 7, 1), 'm')\n",
    "ax.set_xlim(datemin, datemax)\n",
    "\n",
    "# Format the coords message box, i.e. the numbers displayed as the cursor moves\n",
    "# across the axes within the interactive GUI.\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.format_ydata = lambda x: f'${x:.2f}'  # Format the price.\n",
    "ax.grid(True)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Date', ylabel='Weekly total retweet count')\n",
    "ax.set_ylim([0, 1000]) # scale the y-axis range\n",
    "\n",
    "# Rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them.\n",
    "fig.autofmt_xdate()\n",
    "plt.legend() # add the legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e56d2",
   "metadata": {
    "id": "2a5e56d2",
    "outputId": "45898522-445c-4a55-d5cf-8bac48125a2c"
   },
   "outputs": [],
   "source": [
    "# plot number of retweets over time (aggregated over each week) for positive and negative tweets\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "ax.plot(pos_byweek_all[\"date\"], pos_byweek_all['total_retweets'], c='blue', label='Positive tweets')\n",
    "ax.plot(neg_byweek_all[\"date\"], neg_byweek_all['total_retweets'], c='red', label='Negative tweets')\n",
    "\n",
    "\"\"\"\n",
    "# format the vertical lines and annotations\n",
    "y = {\n",
    "  0: 20000,\n",
    "  1: 40000,\n",
    "  2: 30000,\n",
    "  3: 60000,\n",
    "  4: 80000,\n",
    "  5: 50000,\n",
    "  6: 40000,\n",
    "  7: 40000,\n",
    "  8: 60000,\n",
    "  9: 80000,\n",
    "  10: 40000,\n",
    "  11: 140000,\n",
    "  12: 120000,\n",
    "  13: 100000,\n",
    "  14: 160000,\n",
    "  15: 160000,\n",
    "  16: 100000,\n",
    "  17: 120000,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "y_offset = 10000\n",
    "left_align = [1, 4, 5, 8, 9, 10, 15, 16, 17]\n",
    "\n",
    "\"\"\"\n",
    "# vertical line for major events\n",
    "for i in range(len(dates)):\n",
    "  date = dates[i]\n",
    "  event = events[i]\n",
    "  if i in left_align:\n",
    "    align = 'left'\n",
    "  else:\n",
    "    align = 'right'\n",
    "  plt.axvline(x=date, color='grey', linestyle=':')\n",
    "  ax.text(date, y[i]-y_offset, event, ha=align, size=10)\n",
    "\"\"\"\n",
    "\n",
    "# Major ticks every 6 months.\n",
    "fmt_half_year = mdates.MonthLocator(interval=6)\n",
    "ax.xaxis.set_major_locator(fmt_half_year)\n",
    "\n",
    "# Minor ticks every month.\n",
    "fmt_month = mdates.MonthLocator()\n",
    "ax.xaxis.set_minor_locator(fmt_month)\n",
    "\n",
    "# Text in the x axis will be displayed in 'YYYY-mm' format.\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Round to nearest half of year.\n",
    "datemin = np.datetime64(datetime.date(2013, 7, 1), 'm')\n",
    "datemax = np.datetime64(datetime.date(2021, 7, 1), 'm')\n",
    "ax.set_xlim(datemin, datemax)\n",
    "\n",
    "# Format the coords message box, i.e. the numbers displayed as the cursor moves\n",
    "# across the axes within the interactive GUI.\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.format_ydata = lambda x: f'${x:.2f}'  # Format the price.\n",
    "ax.grid(True)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Date', ylabel='Weekly total retweet count')\n",
    "ax.set_ylim([0, 10000]) # scale the y-axis range\n",
    "\n",
    "# Rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them.\n",
    "fig.autofmt_xdate()\n",
    "plt.legend() # add the legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33388935",
   "metadata": {
    "id": "33388935"
   },
   "source": [
    "samething but tweet #: (6b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2b2bc",
   "metadata": {
    "id": "e6b2b2bc",
    "outputId": "06b305ac-e1eb-4c2b-eb4e-034134da69e7"
   },
   "outputs": [],
   "source": [
    "# plot number of retweets over time (aggregated over each week) for positive and negative tweets\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "ax.plot(pos_byweek_tweets_all[\"date\"], pos_byweek_tweets_all['# of tweets'], c='blue', label='Positive tweets')\n",
    "ax.plot(neg_byweek_tweets_all[\"date\"], neg_byweek_tweets_all['# of tweets'], c='red', label='Negative tweets')\n",
    "\n",
    "\"\"\"\n",
    "# format the vertical lines and annotations\n",
    "y = {\n",
    "  0: 20000,\n",
    "  1: 40000,\n",
    "  2: 30000,\n",
    "  3: 60000,\n",
    "  4: 80000,\n",
    "  5: 50000,\n",
    "  6: 40000,\n",
    "  7: 40000,\n",
    "  8: 60000,\n",
    "  9: 80000,\n",
    "  10: 40000,\n",
    "  11: 140000,\n",
    "  12: 120000,\n",
    "  13: 100000,\n",
    "  14: 160000,\n",
    "  15: 160000,\n",
    "  16: 100000,\n",
    "  17: 120000,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "y_offset = 10000\n",
    "left_align = [1, 4, 5, 8, 9, 10, 15, 16, 17]\n",
    "\n",
    "\"\"\"\n",
    "# vertical line for major events\n",
    "for i in range(len(dates)):\n",
    "  date = dates[i]\n",
    "  event = events[i]\n",
    "  if i in left_align:\n",
    "    align = 'left'\n",
    "  else:\n",
    "    align = 'right'\n",
    "  plt.axvline(x=date, color='grey', linestyle=':')\n",
    "  ax.text(date, y[i]-y_offset, event, ha=align, size=10)\n",
    "\"\"\"\n",
    "\n",
    "# Major ticks every 6 months.\n",
    "fmt_half_year = mdates.MonthLocator(interval=6)\n",
    "ax.xaxis.set_major_locator(fmt_half_year)\n",
    "\n",
    "# Minor ticks every month.\n",
    "fmt_month = mdates.MonthLocator()\n",
    "ax.xaxis.set_minor_locator(fmt_month)\n",
    "\n",
    "# Text in the x axis will be displayed in 'YYYY-mm' format.\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Round to nearest half of year.\n",
    "datemin = np.datetime64(datetime.date(2013, 7, 1), 'm')\n",
    "datemax = np.datetime64(datetime.date(2021, 7, 1), 'm')\n",
    "ax.set_xlim(datemin, datemax)\n",
    "\n",
    "# Format the coords message box, i.e. the numbers displayed as the cursor moves\n",
    "# across the axes within the interactive GUI.\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.format_ydata = lambda x: f'${x:.2f}'  # Format the price.\n",
    "ax.grid(True)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Date', ylabel='Weekly total retweet count')\n",
    "ax.set_ylim([0, 2000]) # scale the y-axis range\n",
    "\n",
    "# Rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them.\n",
    "fig.autofmt_xdate()\n",
    "plt.legend() # add the legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2994ec3",
   "metadata": {
    "id": "f2994ec3",
    "outputId": "015f3f8e-d9a4-4b1b-a764-a69ce407f75b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot number of retweets over time (aggregated over each week) for positive and negative tweets\n",
    "fig, ax = plt.subplots(figsize=(30, 8))\n",
    "#ax.plot(pos_byweek_tweets_chopsticks[\"date\"], pos_byweek_tweets_chopsticks['# of tweets'], c='blue', label='Positive tweets')\n",
    "ax.plot(neg_byweek_tweets_all[\"date\"], neg_byweek_tweets_all['# of tweets'], c='red', label='Negative tweets')\n",
    "\n",
    "\"\"\"\n",
    "# format the vertical lines and annotations\n",
    "y = {\n",
    "  0: 20000,\n",
    "  1: 40000,\n",
    "  2: 30000,\n",
    "  3: 60000,\n",
    "  4: 80000,\n",
    "  5: 50000,\n",
    "  6: 40000,\n",
    "  7: 40000,\n",
    "  8: 60000,\n",
    "  9: 80000,\n",
    "  10: 40000,\n",
    "  11: 140000,\n",
    "  12: 120000,\n",
    "  13: 100000,\n",
    "  14: 160000,\n",
    "  15: 160000,\n",
    "  16: 100000,\n",
    "  17: 120000,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "y_offset = 10000\n",
    "left_align = [1, 4, 5, 8, 9, 10, 15, 16, 17]\n",
    "\n",
    "\"\"\"\n",
    "# vertical line for major events\n",
    "for i in range(len(dates)):\n",
    "  date = dates[i]\n",
    "  event = events[i]\n",
    "  if i in left_align:\n",
    "    align = 'left'\n",
    "  else:\n",
    "    align = 'right'\n",
    "  plt.axvline(x=date, color='grey', linestyle=':')\n",
    "  ax.text(date, y[i]-y_offset, event, ha=align, size=10)\n",
    "\"\"\"\n",
    "\n",
    "# Major ticks every 6 months.\n",
    "fmt_half_year = mdates.MonthLocator(interval=6)\n",
    "ax.xaxis.set_major_locator(fmt_half_year)\n",
    "\n",
    "# Minor ticks every month.\n",
    "fmt_month = mdates.MonthLocator()\n",
    "ax.xaxis.set_minor_locator(fmt_month)\n",
    "\n",
    "# Text in the x axis will be displayed in 'YYYY-mm' format.\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "\n",
    "# Round to nearest half of year.\n",
    "datemin = np.datetime64(datetime.date(2013, 7, 1), 'm')\n",
    "datemax = np.datetime64(datetime.date(2021, 7, 1), 'm')\n",
    "ax.set_xlim(datemin, datemax)\n",
    "\n",
    "# Format the coords message box, i.e. the numbers displayed as the cursor moves\n",
    "# across the axes within the interactive GUI.\n",
    "ax.format_xdata = mdates.DateFormatter('%Y-%m')\n",
    "ax.format_ydata = lambda x: f'${x:.2f}'  # Format the price.\n",
    "ax.grid(True)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Date', ylabel='Weekly total retweet count')\n",
    "ax.set_ylim([0, 250]) # scale the y-axis range\n",
    "\n",
    "# Rotates and right aligns the x labels, and moves the bottom of the\n",
    "# axes up to make room for them.\n",
    "fig.autofmt_xdate()\n",
    "plt.legend() # add the legend\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "dolce - 27-8-21.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
