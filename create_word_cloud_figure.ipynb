{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPi-UifXe5Z1"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import spacy\n",
    "from html import unescape\n",
    "from emoji import UNICODE_EMOJI\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pickle\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT140_DATA_DIR = 'Sentiment140.data' # sentiment 140 data set saved here\n",
    "DG_DATA_DIR = 'D_G data' # D&G data set saved here\n",
    "OUTPUT_DIR = 'output' # intermediate output and models saved here\n",
    "FIGURES_DIR = 'figures' # figures saved here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chopsticks = pd.read_csv(os.path.join(DG_DATA_DIR, \"dolcegabbana_chopsticks_mentions_daily_expanded_with_predictions.csv\"))\n",
    "df_all = pd.read_csv(os.path.join(DG_DATA_DIR, \"dolcegabbana_mentions_daily_all_with_predictions.csv\"), lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create D&G count matrices using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for pre-processing/cleaning a tweet\n",
    "def preprocessor(tweet):\n",
    "    tweet = re.sub (r'@[A-Za-z0-9_]+', '_AT_USER_', tweet) # replace @X with _AT_USER_\n",
    "    tweet = re.sub (r'#[A-Za-z0-9_]+', '_HASHTAG_', tweet) # replace #X with _HASTHAG_\n",
    "    tweet = re.sub (r'^RT[\\s]+', '', tweet) # remove RT (retweet) at the start of the tweet\n",
    "    tweet = unescape(tweet) # unescape the HTML\n",
    "    tweet = tweet.lower() # make everything lowercase\n",
    "    return tweet\n",
    "\n",
    "# helper function for tokenization of a tweet\n",
    "def tokenizer(tweet):\n",
    "    tokens = nlp(tweet) # this processes the tweet text  \n",
    "    # only keep tokens (lemmatized) that are alphanumeric (including \"-\" and \"_\") and not a stop word, or represent an emoji\n",
    "    tokens = [t.lemma_ for t in tokens if (re.match(\"^[a-zA-Z0-9_-]*$\", t.text) and not t.is_stop and len(t.text) > 2) or t.text in UNICODE_EMOJI]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chopsticks = list(df_chopsticks['text']) # a list of tweets\n",
    "corpus_all = list(df_all['text']) # a list of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run count vectorizer on D&G chopsticks data set\n",
    "model_chopsticks = CountVectorizer(preprocessor=preprocessor, tokenizer=tokenizer, max_features=2000)\n",
    "word_counts_chopsticks = model_chopsticks.fit_transform(corpus_chopsticks)\n",
    "tokens_chopsticks = model_chopsticks.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save count matrices and tokens/vocab\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'dg_chopsticks_count_vectorizer_output.npz'), word_counts_chopsticks, compressed=True)\n",
    "np.save(os.path.join(OUTPUT_DIR, 'tokens_chopsticks.npy'), tokens_chopsticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run count vectorizer on D&G all data set\n",
    "model_all = CountVectorizer(preprocessor=preprocessor, tokenizer=tokenizer, max_features=2000)\n",
    "word_counts_all = model_all.fit_transform(corpus_all)\n",
    "tokens_all = model_all.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save count matrices and tokens/vocab\n",
    "sparse.save_npz(os.path.join(OUTPUT_DIR, 'dg_all_count_vectorizer_output.npz'), word_counts_all, compressed=True)\n",
    "np.save(os.path.join(OUTPUT_DIR, 'tokens_all.npy'), tokens_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load D&G count matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_chopsticks = sparse.load_npz(os.path.join(OUTPUT_DIR, 'dg_chopsticks_count_vectorizer_output.npz'))\n",
    "tokens_chopsticks = np.load(os.path.join(OUTPUT_DIR, 'tokens_chopsticks.npy'), allow_pickle=True)\n",
    "\n",
    "word_counts_all = sparse.load_npz(os.path.join(OUTPUT_DIR, 'dg_all_count_vectorizer_output.npz'))\n",
    "tokens_all = np.load(os.path.join(OUTPUT_DIR, 'tokens_all.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cn-abYes_fO"
   },
   "source": [
    "# Separate into Positive and Negative Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7C1_zJAltDZ8"
   },
   "outputs": [],
   "source": [
    "# split into positive and negative dataframes using ensemble of classifiers (svc, rfc, lr, nb)\n",
    "df_all_pos = df_all[df_all['pred'] == 1]\n",
    "df_all_neg = df_all[df_all['pred'] == -1]\n",
    "\n",
    "df_chopsticks_pos = df_chopsticks[df_chopsticks['pred'] == 1]\n",
    "df_chopsticks_neg = df_chopsticks[df_chopsticks['pred'] == -1]\n",
    "\n",
    "# find the index of the positive and negative tweets\n",
    "df_all_pos_ind = np.array(df_all_pos.index)\n",
    "df_all_neg_ind = np.array(df_all_neg.index)\n",
    "\n",
    "df_chopsticks_pos_ind = np.array(df_chopsticks_pos.index)\n",
    "df_chopsticks_neg_ind = np.array(df_chopsticks_neg.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get positive and negative word frequencies for D&G all data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FygsZdBMduEQ"
   },
   "outputs": [],
   "source": [
    "# get the total frequency of each word by summing counts across all tweets for a given word\n",
    "total_freq_all = word_counts_all.sum(axis=0)\n",
    "\n",
    "# use the indices to segment the sparse matrix of filtered word counts \n",
    "pos_word_counts_all = word_counts_all[df_all_pos_ind,:]\n",
    "neg_word_counts_all = word_counts_all[df_all_neg_ind,:]\n",
    "\n",
    "# the positive frequency of a word is its total count in positive tweets subtracted by its total count in negative tweets\n",
    "pos_freq_all = (pos_word_counts_all.sum(axis=0) - neg_word_counts_all.sum(axis=0)).tolist()[0]\n",
    "# the negative frequency of a word is its total count in negative tweets subtracted by its totla count in positive tweets\n",
    "neg_freq_all = (neg_word_counts_all.sum(axis=0) - pos_word_counts_all.sum(axis=0)).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlHJYyZHelVO"
   },
   "outputs": [],
   "source": [
    "# create dictionaries where key is the word and value is its frequency (negative or positive)\n",
    "pos_freq_list = dict(zip(tokens_all, pos_freq_all))\n",
    "neg_freq_list = dict(zip(tokens_all, neg_freq_all))\n",
    "\n",
    "# sort based on the frequencies\n",
    "pos_sorted = sorted(pos_freq_list, key = lambda x: x[1], reverse=True)\n",
    "neg_sorted = sorted(neg_freq_list, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1628656107418,
     "user": {
      "displayName": "Jackie Peng",
      "photoUrl": "",
      "userId": "16149687417498493859"
     },
     "user_tz": 240
    },
    "id": "YoAsyGOkejMN",
    "outputId": "f57e053d-3053-4dd3-da5c-5d6752c9105c"
   },
   "outputs": [],
   "source": [
    "num_words_to_filter = 200 # number of words to plot in each word cloud\n",
    "\n",
    "# get the top 200 words based on positive and negative frequency\n",
    "top_pos_words = [i[0] for i in pos_sorted][:num_words_to_filter] \n",
    "top_neg_words = [i[0] for i in neg_sorted][:num_words_to_filter] \n",
    "\n",
    "# check if there are overlapping words in top_pos_words and top_neg_words\n",
    "same_words = set(set(top_pos_words).intersection(set(top_neg_words)))\n",
    "print(len(same_words), 'words overlap between top neg and pos words', num_words_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "executionInfo": {
     "elapsed": 3103,
     "status": "ok",
     "timestamp": 1628656110514,
     "user": {
      "displayName": "Jackie Peng",
      "photoUrl": "",
      "userId": "16149687417498493859"
     },
     "user_tz": 240
    },
    "id": "G4JBt_9sf45K",
    "outputId": "9181debd-9a71-4c5e-a22d-ab0481cfaf77"
   },
   "outputs": [],
   "source": [
    "## plot word clouds\n",
    "\n",
    "# positive wordcloud\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20, 16))\n",
    "\n",
    "# positive words w overlap\n",
    "wordcloud = WordCloud(width = 400, \n",
    "                      height = 300, \n",
    "                      random_state = 42,\n",
    "                      max_words = 200,\n",
    "                      background_color = 'black')\n",
    "wordcloud.generate_from_frequencies(pos_freq_list) # generate from CountVectorizer frequencies\n",
    "ax1.imshow(wordcloud)\n",
    "ax1.set_title('Positive words', fontsize = 20)\n",
    "ax1.axis(\"off\")\n",
    "ax1.text(0, 0, 'A)', fontsize=30)\n",
    "\n",
    "# negative w overlap\n",
    "wordcloud = WordCloud(width = 400, \n",
    "                      height = 300, \n",
    "                      random_state = 42,\n",
    "                      max_words = 200,\n",
    "                      background_color = 'black')\n",
    "wordcloud.generate_from_frequencies(neg_freq_list) # generate from CountVectorizer frequencies\n",
    "ax2.imshow(wordcloud)\n",
    "ax2.set_title('Negative words', fontsize = 20)\n",
    "ax2.text(0, 0, 'B)', fontsize=30)\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "#plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.1, hspace=None)\n",
    "\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'D&G_all_wordcloud_pos_vs_neg.jpg'), dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1628656110515,
     "user": {
      "displayName": "Jackie Peng",
      "photoUrl": "",
      "userId": "16149687417498493859"
     },
     "user_tz": 240
    },
    "id": "eaFKdKDPoINQ",
    "outputId": "f6ca618a-f095-4b85-cf4a-eeb9b4cbe19e"
   },
   "outputs": [],
   "source": [
    "# list out the top positive words and their frequencies\n",
    "top_k = 25\n",
    "pos_freq_list_sorted = sorted(pos_freq_list.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, tup in enumerate(pos_freq_list_sorted):\n",
    "    if i == top_k:\n",
    "        break\n",
    "\n",
    "    print(f'{i+1}. {tup[0]} ({tup[1]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1628656110516,
     "user": {
      "displayName": "Jackie Peng",
      "photoUrl": "",
      "userId": "16149687417498493859"
     },
     "user_tz": 240
    },
    "id": "0ZxKt8WMngtn",
    "outputId": "cdfd631d-e764-40ee-d924-f0428df14920"
   },
   "outputs": [],
   "source": [
    "# list out the top negative words and their frequencies\n",
    "neg_freq_list_sorted = sorted(neg_freq_list.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, tup in enumerate(neg_freq_list_sorted):\n",
    "    if i == top_k:\n",
    "        break\n",
    "        \n",
    "    print(f'{i+1}. {tup[0]} ({tup[1]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQGG2KLpnTD5"
   },
   "source": [
    "# Get positive and negative word frequencies for D&G chopsticks data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FygsZdBMduEQ"
   },
   "outputs": [],
   "source": [
    "# get the total frequency of each word by summing counts across all tweets for a given word\n",
    "total_freq_chopsticks = word_counts_chopsticks.sum(axis=0)\n",
    "\n",
    "# use the indices to segment the sparse matrix of filtered word counts \n",
    "pos_word_counts_chopsticks = word_counts_chopsticks[df_chopsticks_pos_ind,:]\n",
    "neg_word_counts_chopsticks = word_counts_chopsticks[df_chopsticks_neg_ind,:]\n",
    "\n",
    "# the positive frequency of a word is its total count in positive tweets subtracted by its total count in negative tweets\n",
    "pos_freq_chopsticks = (pos_word_counts_chopsticks.sum(axis=0) - neg_word_counts_chopsticks.sum(axis=0)).tolist()[0]\n",
    "# the negative frequency of a word is its total count in negative tweets subtracted by its totla count in positive tweets\n",
    "neg_freq_chopsticks = (neg_word_counts_chopsticks.sum(axis=0) - pos_word_counts_chopsticks.sum(axis=0)).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlHJYyZHelVO"
   },
   "outputs": [],
   "source": [
    "# create dictionaries where key is the word and value is its frequency (negative or positive)\n",
    "pos_freq_list = dict(zip(tokens_chopsticks, pos_freq_chopsticks))\n",
    "neg_freq_list = dict(zip(tokens_chopsticks, neg_freq_chopsticks))\n",
    "\n",
    "# sort based on the frequencies\n",
    "pos_sorted = sorted(pos_freq_list, key = lambda x: x[1], reverse=True)\n",
    "neg_sorted = sorted(neg_freq_list, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1628656107418,
     "user": {
      "displayName": "Jackie Peng",
      "photoUrl": "",
      "userId": "16149687417498493859"
     },
     "user_tz": 240
    },
    "id": "YoAsyGOkejMN",
    "outputId": "f57e053d-3053-4dd3-da5c-5d6752c9105c"
   },
   "outputs": [],
   "source": [
    "num_words_to_filter = 200 # number of words to plot in each word cloud\n",
    "\n",
    "# get the top 200 words based on positive and negative frequency\n",
    "top_pos_words = [i[0] for i in pos_sorted][:num_words_to_filter] \n",
    "top_neg_words = [i[0] for i in neg_sorted][:num_words_to_filter] \n",
    "\n",
    "# check if there are overlapping words in top_pos_words and top_neg_words\n",
    "same_words = set(set(top_pos_words).intersection(set(top_neg_words)))\n",
    "print(len(same_words), 'words overlap between top neg and pos words', num_words_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "executionInfo": {
     "elapsed": 3103,
     "status": "ok",
     "timestamp": 1628656110514,
     "user": {
      "displayName": "Jackie Peng",
      "photoUrl": "",
      "userId": "16149687417498493859"
     },
     "user_tz": 240
    },
    "id": "G4JBt_9sf45K",
    "outputId": "9181debd-9a71-4c5e-a22d-ab0481cfaf77"
   },
   "outputs": [],
   "source": [
    "## plot word clouds\n",
    "\n",
    "# positive wordcloud\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize = (20, 16))\n",
    "\n",
    "# positive words w overlap\n",
    "wordcloud = WordCloud(width = 400, \n",
    "                      height = 300, \n",
    "                      random_state = 42,\n",
    "                      max_words = 200,\n",
    "                      background_color = 'black')\n",
    "wordcloud.generate_from_frequencies(pos_freq_list) # generate from CountVectorizer frequencies\n",
    "ax1.imshow(wordcloud)\n",
    "ax1.set_title('Positive words', fontsize = 20)\n",
    "ax1.axis(\"off\")\n",
    "ax1.text(0, 0, 'A)', fontsize=30)\n",
    "\n",
    "# negative w overlap\n",
    "wordcloud = WordCloud(width = 400, \n",
    "                      height = 300, \n",
    "                      random_state = 42,\n",
    "                      max_words = 200,\n",
    "                      background_color = 'black')\n",
    "wordcloud.generate_from_frequencies(neg_freq_list) # generate from CountVectorizer frequencies\n",
    "ax2.imshow(wordcloud)\n",
    "ax2.set_title('Negative words', fontsize = 20)\n",
    "ax2.text(0, 0, 'B)', fontsize=30)\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "#plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.1, hspace=None)\n",
    "\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'D&G_chopsticks_wordcloud_pos_vs_neg.jpg'), dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1628656110515,
     "user": {
      "displayName": "Jackie Peng",
      "photoUrl": "",
      "userId": "16149687417498493859"
     },
     "user_tz": 240
    },
    "id": "eaFKdKDPoINQ",
    "outputId": "f6ca618a-f095-4b85-cf4a-eeb9b4cbe19e"
   },
   "outputs": [],
   "source": [
    "# list out the top positive words and their frequencies\n",
    "top_k = 25\n",
    "pos_freq_list_sorted = sorted(pos_freq_list.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, tup in enumerate(pos_freq_list_sorted):\n",
    "    if i == top_k:\n",
    "        break\n",
    "\n",
    "    print(f'{i+1}. {tup[0]} ({tup[1]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1628656110516,
     "user": {
      "displayName": "Jackie Peng",
      "photoUrl": "",
      "userId": "16149687417498493859"
     },
     "user_tz": 240
    },
    "id": "0ZxKt8WMngtn",
    "outputId": "cdfd631d-e764-40ee-d924-f0428df14920"
   },
   "outputs": [],
   "source": [
    "# list out the top negative words and their frequencies\n",
    "neg_freq_list_sorted = sorted(neg_freq_list.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, tup in enumerate(neg_freq_list_sorted):\n",
    "    if i == top_k:\n",
    "        break\n",
    "        \n",
    "    print(f'{i+1}. {tup[0]} ({tup[1]})')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "pos_neg_word_cloud_blm_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
